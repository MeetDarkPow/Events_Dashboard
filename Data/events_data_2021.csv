Year,Subject,Calendar Name,Created By,Updated By,Who,Location,Description,Tags,Session name
2021,R for Non-Programmers: Creating Paradigm Shifts in Reporting for Community-Facing Organizations Using R,Regular talks > Track A,UseR! 2021,UseR! 2021,"Kulka, Lisa",The Lounge #talk_community_outreach_1,"The automation of reporting processes for community-based organizations working with diverse communities has created a paradigm shift in the way they can approach quality improvement, allocate time and resources to data analyses and management, and utilize various kinds of data to support the communities with which they work.

  
At CCNY, a nonprofit organization that supports the evaluation and analytics work of community-facing organizations, the use of R to generate and enhance reporting schema has made a significant impact for those who have little to no programming experience. The most successful projects leading to increased organizational effectiveness via use of R with non-programmers include CCNY’s support of its local county’s Children’s System of Care. Non-automated data reporting and management created challenges in terms of credential tracking and ensuring children and families were receiving appropriate services. CCNY deployed R to pre-process and automate training data, generating reports that predict which community providers are eligible to render services. This new organizational ability created streamlined reporting processes that led to non-programmers running R and applying this newly-established data framework to automate other data-related tasks, including dramatically increasing community impact by reducing data processing time, making faster decisions informed by real-time data, and leveraging increased data processing capabilities to improve overall organizational capacity.

  
In this session, we will review specific features of the project’s unique code used to establish streamlined automated reporting for those with limited R proficiency, as well as the project input, the output deliverables, and techniques for engaging non-programmers in the logistics of building R schema so that basic principles of automated reporting can be understood and easily generalized within and outside of community-facing organizations.",Community and Outreach,1A - Community and Outreach 1
2021,"RcppDeepState, a simple way to fuzz test R packages.",Regular talks > Track B,UseR! 2021,UseR! 2021,"Chowdary Kolla, Akhila",The Lounge #talk_packages_1,"R packages are typically tested using expected input/output pairs that are manually coded by package developers. These manually written tests are validated under various CRAN checks that perform both static and dynamic analysis. Manually written tests could still allow subtle bugs, if they do not anticipate all possible inputs, and miss important code paths. In contrast to manually written tests, fuzzers are programs that pass random, unexpected, potentially invalid inputs to a function, expecting it to break or identify subtle bugs.","Interfaces with other programming languages, Other, R in production",1B - R packages 1
2021,rOpenSci’s Model for Managing a Federated Open Source Software Community,Regular talks > Track A,UseR! 2021,UseR! 2021,"Butland, Stefanie",The Lounge #talk_community_outreach_1,"rOpenSci hosts over 350 staff- and community-contributed R packages. We have evolved a unique model of community management to support the complex needs of people who develop, review, and use these packages.

  
The Community Participation Model from the Center for Scientific Collaboration and Community Engagement (<http://doi.org/10.5281/zenodo.3997802>) provides a framework to assess how community members interact with programs and each other. The four modes on a continuum are: Convey/Consume; Contribute; Collaborate; Co-create. Engagement among rOpenSci community members happens primarily in the first three modes. 1) Convey/Consume. One-way dissemination of information via a newsletter, and Community Contributing Guide (<https://contributing.ropensci.org/>) that helps people match their motivations and skills to different ways to contribute. 2) Contribute. Opportunities for members to share knowledge e.g. via our package development guide (<https://devguide.ropensci.org/>), and blog posts written by members that draw attention to their work. 3) Collaborate. Scaffolded activities where members work together e.g. in open software peer-review as authors, reviewers, or editors. Additional programming, such as community calls, provides opportunities for multiple modes of participation at once e.g. by collaborating on developing topics and presenting, contributing by sharing resources and questions and consuming by attending presentations or reading the recaps.

  
Strong social facilitation by a full-time community manager and a team that values trust-based relationships supports community members in modes 1 - 3. We have recently explored what might be required to provide more opportunities for co-creation - mode 4 in the CSCCE Community Participation Model. An interviews-based assessment of community needs and audit of current programming aims to help us understand what people get from the rOpenSci community that they can’t get elsewhere, and how we can best facilitate productive and valuable collaboration and co-creation. We’ll present the results of this work as a methodology for others to consider how to review their own community engagement activities.",Community and Outreach,1A - Community and Outreach 1
2021,R in Regulated Industries: Assessing Risk with {riskmetric},Regular talks > Track B,UseR! 2021,UseR! 2021,"Kelkhoff, Doug",The Lounge #talk_packages_1,"Regulated industries are built on long histories of software validation to bring confidence to the results that they produce. Historically, this validation has come through licensed software — at a time, such software tools were the preferred tool of practitioners. As free and open source tools rise in popularity, the spectrum of preferred tools grows ever wider. Today, R is the preferred language for many graduating statisticians and biologists. Furthermore, the open nature of these tools brings more ready access to bleeding edge methods expected by regulators.

  
While the R world brings some amazing pieces of infrastructure for software quality, there is an inevitable gap in translating open source practices to a validation world that is unfamiliar with open software development. With {riskmetric}, we have built a platform for implementing such quality assessments. From code quality to community engagement, we hope to characterize R packages to an extent that industry players can be confident that they can stand behind the tools they use when presenting analysis and software to regulators.

  
Here we present the {riskmetric} package and surrounding tools to help support R package validation, show how we’ve built a foundation for pulling metrics from a diverse set of information sources and provide next steps in the R Validation Hub’s roadmap to help bridge the gap toward open source tools in regulated industries.","Interfaces with other programming languages, Other, R in production",1B - R packages 1
2021,R Developer's Guide,Regular talks > Track A,UseR! 2021,UseR! 2021,"Bhogal, Saranjeet Kaur Satnam Singh",The Lounge #talk_community_outreach_1,"The R Developer's Guide (https://github.com/forwards/rdevguide) is an open-source project that aims to facilitate the on-boarding of new contributors to R Core development. New contributors to R are often not aware of where they can start contributing to the development of R. This guide helps in providing ways by which you could start contributing. How would you report a bug? What is the procedure to submit a patch? How could you help improve the documentation? These are some of the questions which new contributors may not have an idea about. In this talk, I will discuss these procedures. I will also walk through this guide for new contributors interested in referring to it for their contributions.",Community and Outreach,1A - Community and Outreach 1
2021,{poorman} - A dependency free recreation of {dplyr},Regular talks > Track B,UseR! 2021,UseR! 2021,"Eastwood, Nathan Wayne",The Lounge #talk_packages_1,"{poorman} is a package that unapologetically attempts to recreate the {dplyr} API in a dependency free way using only {base} R. {poorman} is still under development and doesn’t have all of {dplyr}’s functionality but what is considered the """"core"""" functionality is included and the package is available on CRAN at: <https://cran.r-project.org/web/packages/poorman/>. The idea behind {poorman} is that a user should be able to take a {dplyr} based script and run it using {poorman} without any hiccups.

{poorman} provides a consistent set of verbs that help you solve the most common data manipulation challenges:

- select() picks variables based on their names.
- mutate() adds new variables that are functions of existing variables.
- filter() picks cases based on their values.
- summarise() reduces multiple values down to a single summary.
- arrange() changes the ordering of the rows.","Interfaces with other programming languages, Other, R in production",1B - R packages 1
2021,Packages submission and reviews; how does it work?,Regular talks > Track A,UseR! 2021,UseR! 2021,"Revilla Sancho, Lluís",The Lounge #talk_community_outreach_1,"We benefit from others’ work on R and also by shared packages and for our programming tasks. Occasionally we might generate some piece of software that we want to share with the community. Usually sharing our work with the R community means submitting a package to an archive (CRAN, Bioconductor or others). While each individual archive has some rules they share some common principles.

  
If your package follows their rules about the submission process and has a good quality according to their rules it will be included. All submissions have some common sections: First, an initial screening; second, a more profound manual review of the code. Then, if the suggestions are applied or correctly replied then the package is included in the archive.

  
On each step some rules and criteria are used to decide if the package moves forward or not. Understanding what these rules say, common problems and comments from reviewers will help avoiding submitting a package to get it rejected. Reducing the friction between sharing our work, providing useful packages to the community and minimizing reviewers’ time and efforts.

  
Looking at the review process of three archives of R packages, CRAN, Bioconductor and rOpenSci, I’ll explain common rules, patterns, timelines and checks required to get the package included, as well as personal anecdotes with them. The talk is based on the post analyzing reviews available here: <https://llrs.dev/tags/reviews/>",Community and Outreach,1A - Community and Outreach 1
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_packages_1,Last questions for all the speakers,Community and Outreach,1B - R packages 1
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_community_outreach_1,Last questions for all the speakers,Community and Outreach,1A - Community and Outreach 1
2021,Solving Big Data Problems with Apache Arrow,Regular talks > Track A,UseR! 2021,UseR! 2021,"Richardson, Neal",The Lounge #talk_data_management,"As distributed computing platforms and data warehouses become more prevalent, R users face new challenges in working with the data they generate and store. From R, we may need to analyze a dataset that is too big to fit into memory, is split across many files, is hosted on cloud storage, or was produced by systems using other languages. The {arrow} package helps to solve these integration problems, allowing R users to employ familiar, idiomatic R code to work with larger-than-memory datasets on their own workstations. This presentation will discuss several of these common challenges people face when working with bigger data, and using case studies from the community, it will demonstrate how Arrow can help solve these problems.",Databases / Data management,2A - Data Management
2021,All-in-one smartphone-based system for quantitative analysis of point-of-care diagnostics,Regular talks > Track B,UseR! 2021,UseR! 2021,"Schary, Weronika; Paskali, Filip",The Lounge #talk_shiny,"We propose a smartphone-based system for the quantification of various lateral flow assays for the detection and diagnosis of diseases. The proposed smartphone-based system consists of a 3D-printed photo box for standardized positioning and lighting, a smartphone for image acquisition and an R Shiny Software Package with modular, customizable applications for image editing, analysis, data extraction, calibration and quantification. This system is less expensive than commonly used hardware and software for analysis, so it could prove very beneficial for diagnostic testing in the context of pandemics, as well as low-resource countries, in which laboratory equipment as well as diagnostic facilities are scarce.

  
The proposed system is facilitated with R Shiny, an open-source package - free to use and modify. It can be used without extensive programming skills, which could further the development of diagnosis becoming simpler, quicker, more efficient and still cost-effective compared to the gold standard methods used in detection and diagnosis today. Also, the automatic documentation of all analysis steps implemented in the application via R Markdown allows for accurate reproducibility in research and clinical practice.

  
For further image analysis, package LFApp was created to enable image editing, cropping, segmentation, background correction, data analysis, calibration and quantification of extracted pixel intensity values from the image.

  
Besides Shiny, other major packages used are EBImage, ggplot2, DT, shinyjs, stats, shinyFiles, rmarkdown and shinythemes. Furthermore, we designed an additional version of the UI module, using ShinyMobile, to make the app more accessible on small touchscreens.

  
Our goal was to build a versatile free open-source system, that is scalable and extensible, and also modifiable to suit any research team requirements. It represents an all-in-one, portable, cost-efficient and easily reproducible system for full analysis, that works well on computers as well as portable devices, such as smartphones.",Web Applications (Shiny/Dash),2B - Shiny
2021,respectables and synthetic.cdisc.data: A framework for creating relational data with application,Regular talks > Track A,UseR! 2021,UseR! 2021,"Becker, Gabriel",The Lounge #talk_data_management,"Synthetic data provides a privacy-safe mechanism for developing, benchmarking, testing, and showcasing analysis plans and data processing pipelines. Existing tools in R focus primarily on creating or manipulating individual rectangular datasets (dplyr) or combining already existing multiple rectangular datasets (dm). Many crucial types of data, however, involve inter-related rectangular sets of data, with columns in one table acting as keys or lookups within another. An example of this is the CDISC data standard for clinical trial data, which, given an overall set of patients, has some rectangular datasets containing exactly one row per patient and other datasets where some patients have data represented in multiple rows while other patients are entirely absent (e.g., because they did not have any adverse events).

  
The respectables package defines a recipe-based framework which allows for the specification of data to be synthesized at three distinct levels. First, we provide an intuitive recipe mechanism for specifying the creation - via sampling, synthesis or both - of a rectangular dataset. These recipes support the definition of both conditional and joint behaviors between sets of variables. Second, we define the concept of a scaffolding join recipe which specifies the creation of a rectangular dataset with a particular foreign-key style relationship with another dataset. Finally, we combine these two types of recipes to create recipe books which specify, a priori, the creation and construction of full database-like cohorts of inter-related datasets, either based on existing starting data or from whole cloth.

  
The synthetic.cdisc.data package provides respectables recipes for the creation of synthetic clinical trial readout data that adheres to the CDISC standard.

  
respectables and synthetic.cdisc.data will be released open source - approval granted prior to abstract submission - and available on github at the time of the presentation.",Databases / Data management,2A - Data Management
2021,Unit Testing Shiny App Reactivity,Regular talks > Track B,UseR! 2021,UseR! 2021,"Sidi, Jonathan",The Lounge #talk_shiny,"When developing Shiny apps there are a lot of reactivity problems that can arise when one reactive or observe element triggers other elements. In some cases these can create cascading reactivity (the horror). The goal of reactor is to efficiently diagnose these reactivity problems and then plan unit tests to avert them during app development, making it a less painful and more robust experience. Reactor can improve the stability of shiny app development with many collaborators through its application in a version control and CI/CD framework.",Web Applications (Shiny/Dash),2B - Shiny
2021,Going Big and Fast - {kafkaesque} for Kafka Access,Regular talks > Track A,UseR! 2021,UseR! 2021,"Meißner, Peter",The Lounge #talk_data_management,"Kafka is a big data technology that allows for high throughput low latency stream processing, storing and distributing data. Kafka is written in Java and has become an infrastructure industry standard to for real time, near time, microservice and distributed applications.

  
This talk introduces {kafkaesque} a package which allows R users to integrate their code and models with Kafka e.g., to use it as a distributed message queue or to access and process data fed into Kafka by other systems. Besides presenting core concepts and how to use the package, the talk will also talk about the development process and the pros and cons of using Java code in R packages.",Databases / Data management,2A - Data Management
2021,ShinyQuickStarter: Build Shiny apps interactively with Drag & Drop,Regular talks > Track B,UseR! 2021,UseR! 2021,"Binder, Leon",The Lounge #talk_shiny,"The development of Shiny apps is often very time-consuming. This applies to the initial setup of the folder structure, but especially to the creation of the user interface and the implementation of the program logic. Many UI and logic elements are available and distributed across several packages.

  
To make the development process more efficient, we developed the RStudio addin ‘ShinyQuickStarter’. ‘ShinyQuickStarter’ is designed both for beginners who have acquired some basic knowledge of Shiny but have not gained much practical experience, and for advanced users who want to accelerate the process of developing new, powerful Shiny apps. It helps to setup the design of Shiny apps within a few minutes, so developers can start implementing the actual program logic almost immediately.

  
‘ShinyQuickStarter’ enables developers to create Shiny apps interactively using an intuitive drag and drop interface. A variety of page types and over 75 UI elements for navigation, layout, inputs, and outputs are supported. The options of these UI elements are interactively customizable, so developers can easily tailor Shiny apps towards their requirements and see the effect of an option immediately. Context-sensitive documentation furthermore supports especially beginners in orchestrating easy-to-use Shiny apps. ‘ShinyQuickStarter’ also creates the required folder structure for a new app and exports the source code of both the UI and the server component. Developers have the opportunity to organize the source code into Shiny modules. ‘ShinyQuickStarter’ solves the core problem of creating Shiny apps by streamlining the workflow of creating UI elements and corresponding server-side elements.",Web Applications (Shiny/Dash),2B - Shiny
2021,Scaling R for Enterprise Data,Regular talks > Track A,UseR! 2021,UseR! 2021,"Hornick, Mark",The Lounge #talk_data_management,"While R has made significant gains in performance with each new release, sometimes the data itself is the bottleneck - having to move it from one environment to another. With enterprise data stored in Oracle databases, the integration of R with Oracle Database enables using R more efficiently at on larger volume data. Oracle Machine Learning for R (OML4R) – the R interface to in-database machine learning and R script deployment from Oracle – enables you to work with database tables and views using familiar R syntax and functions. For scalable and performant data exploration, data preparation, and machine learning, R users leverage Oracle Database as a high-performance compute engine and build machine learning models using parallelized in-database algorithms using R Formula-based specification. Further, deployment of user-defined R functions from SQL facilitates application and dashboard development, where R engines are dynamically spawned and controlled by Oracle Database. Users can even take advantage of running user-defined R functions in a data-parallel and task-parallel manner.",Databases / Data management,2A - Data Management
2021,The ShinyProxy Good News Show,Regular talks > Track B,UseR! 2021,UseR! 2021,"De Koninck, Tobia",The Lounge #talk_shiny,"Interactive web applications have become standard data science artefacts. Since 5 years ShinyProxy offers a 100% open source enterprise solution to run and manage such applications. Whether you want to deploy Shiny, Dash, H2O Wave or Streamlit apps, ShinyProxy has your back. Whether you serve a small team or host internet-facing apps for thousands of users, ShinyProxy will scale and stand the load. Whether you use LDAP, ActiveDirectory, OpenID Connect, SAML 2.0 or Kerberos to authenticate / authorize users, ShinyProxy makes it happen. Want to mix in an IDE (e.g. RStudio) or notebook server (e.g. Jupyter or Zeppelin notebooks)? Look no further than ShinyProxy. Monitor your stack and gather usage statistics? Check. Embed the apps over APIs into other websites? Solved problem. In this talk we provide technical detail to the good news and focus on the latest ShinyProxy refinements and developments.",Web Applications (Shiny/Dash),2B - Shiny
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_data_management,Last questions for all the speakers,Community and Outreach,2A - Community and Outreach 1
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_shiny,Last questions for all the speakers,Community and Outreach,2B - Shiny
2021,Incubator: Five principles to grow up your R community,Panels and incubators,UseR! 2021,UseR! 2021,"Chávez-Fuentes, Joselyn C.; Cuevas-Fernández, Erick; Reyes, Alejandro; Collado-Torres, Leonardo",The Lounge #incubator_r_community,"The Community of Bioinformatics Software Developers CDSB (<https://comunidadbioinfo.github.io>) was created in 2018 with the aim to promote R software development in México, increase Latin American representation in global communities, and facilitate the transition from software users to software developers. Through CDSB, we encourage users to develop and present R packages at international conferences, such as useR, RStudio, Bioconductor, LatinR, and ConectaR. Three years after its creation, the CDSB community has members from all over the country as well as from other Latin American countries, e.g. Costa Rica, Perú, Ecuador, Colombia. Importantly, this community has served as a base for CDSB members to create their own local communities, like R-Ladies Chapters (<https://rladiesmx.netlify.app>). In this incubator session, we will share five key principles that have helped us grow as a community, and how you can apply them in your local community.",Community and Outreach,Incubator: Five principles to grow up your R community
2021,Elevator Pitches 1,Elevator Pitches,UseR! 2021,UseR! 2021,,The Lounge #elevator_pitches,"Elevator pitches (short videos and technical notes):

1. Hetu-package: Validating and Extracting Information from Finnish National Identification Numbers (Pyry Kantanen, Måns Magnusson, Jussi Paananen, Leo Lahti)
2. Visualising variable importance and variable interaction effects in machine learning models. (Alan Inglis, Andrew Parnell, Catherine Hurley)
3. Obtaining reproducible reports on satellite hotspot data during a wildfire disaster (Natalia Soledad Morandeira)
4. Modeling spatio-temporal point processes with nphawkes package (Peter Boyd, Dr. James Molyneux)
5. Use Case: Functional Programming and Parallelization in Spatial Point Pattern Analysis (Clara Chua, Tin Seong Kam)
6. Predicting the COVID-19 Pandemic Impact on Clinical Trial Recruitment at GSK (Valeriia Sherina, Nicky Best, Graeme Archer, Jack Euesden, Dave Lunn, Inna Perevozskaya, Doug Thompson, Magda Zwierzyna)
7. The Grammar of Experimental Design (Emi Tanaka)
8. rspatialdata: a collection of data sources and tutorials on downloading and visualising spatial data using R (Varsha Ujjini Vijay Kumar, Dilinie Seimon, Paula Moraga)
9. tramME: Mixed-Effects Transformation Models Using Template Model Builder (Balint Tamasi, Torsten Hothorn)
10. Teaching Advanced Data Science in R: Successes and Failures (Lisa Lendway)
11. rRbiMs, r tools for Reconstructing bin Metabolisms. (Mirna Vázquez Rosas Landa, Valerie de Anda Torres, Sahil Shan, Brett Baker)
12. ResidualRWA: Detecting relevant variable using relative weight analysis with residualization (Maikol Solís, Carlos Pasquier)
13. serosurvey R package: Serological Survey Analysis For Prevalence Estimation Under Misclassification (Andree Valle-Campos)
14. httrex: Help Debugging HTTP API Clients (Greg Daniel Freedman Ellis)
15. Reading, Combining, and Pre-Filtering Files with R and AWK (David Shilane, Mayur Bansal, Chung Woo Lee)
16. DATA PIPELINE: IMPROVING MANAGEMENT OF FINANCIAL CONTRIBUTIONS TO THE FIGHT AGAINST POVERTY IN COSTA RICA (Roberto Delgado Castro)
17. mctq: An R Package for the Munich ChronoType Questionnaire (Daniel Vartanian, Ana Amélia Benedito-Silva, Mario Pedrazzoli)
18. {flowDiv} workflow: reproducible cytometric diversity estimates (María Victoria Quiroga, Bruno M. S. Wanderley, André M. Amado, Fernando Unrein)
19. From Sheets to Success: Reliable, Reproducible, yet Flexible Production Pipelines (Katrina Brock)
20. Efficient multi-wave sampling with the R package optimall (Jasper B. Yang, Bryan E. Shepherd, Thomas Lumley, Pamela A. Shaw)
21. Open the Machine Learning Black-Box with modelStudio &amp; Arena (Hubert Baniecki, Piotr Piątyszek)
22. Classifying Student’s Learning Pattern using R Sequence Analysis Packages: The Impact of Procrastination on Performance (Teck Kiang Tan)
23. Outlier redemption (Violeta Roizman)
24. The Canyon of Success: Scaling Best Practices in R with Internal R packages (Malcolm Barrett)
25. Modeling tumor evolutionary dynamics with SITH (Phillip Nicol, Amir Asiaee)
26. A Glimpse into the Reproduciblity of Scientific Papers published in Movement Ecology: How are we doing ? (Jenicca Poongavanan, Rocio Joo Arakawa, Mathieu Basille)
27. Forecasting under COVID: when simple works - and when it doesn’t (Maryam Shobeirinejad, Steph Stammel)
28. gatpkg: Developing a geographic aggregation tool in R for non-programmers (Abigail Stamm)
29. tidyndr: An R package for analysis of the Nigeria HIV National Data Repository (Stephen Taiye Balogun, Scholastica Olanrewaju, Oluwaseun Okunuga, Temitope Kolade, Geraldine Chizoba Abone, Fati Murtala-Ibrahim, Helen Omuh)
30. Do you see what I see? Introducing microshades: An R package for improving color accessibility and organization of complex data (Lisa Karstens, Erin Dahl, Emory Neer)",EP,EP1
2021,mlr3automl - Automated Machine Learning in R,Regular talks > Track A,UseR! 2021,UseR! 2021,"Hanf, Alexander Bernd",The Lounge #talk_ml_dm,"We introduce mlr3automl, an open-source framework for Automated Machine Learning in R. Based on the mlr3 Machine Learning package, mlr3automl builds robust and accurate classification and regression models for tabular data.

  
mlr3automl provides automatic preprocessing, which guarantees stable performance in the presence of missing data, categorical and high-cardinality features, and large data sets. Preprocessing and model building is solved through a flexible pipeline implemented with mlr3pipelines. This allows mlr3automl to jointly optimize preprocessing, model selection and model hyperparameters using Hyperband.

  
mlr3automl shows strong performance and stability on a benchmark consisting of 39 challenging classification tasks. mlr3automl successfully completed every task in the benchmark within the strict time budget, which three out of five other state of the art AutoML systems failed to achieve.",Data mining / Machine learning / Deep Learning and AI,3A - Machine Learning and Data Management
2021,Tidy Geospatial Networks in R,Regular talks > Track B,UseR! 2021,UseR! 2021,"van der Meer, Lucas",The Lounge #talk_spatial_analysis,"Geospatial networks are graphs embedded in geographical space. That means that both the nodes and edges in the graph can be represented as geographic features: the nodes most commonly as points, and the edges as linestrings. They play an important role in many different domains, ranging from transportation planning and logistics to ecology and epidemiology. The structure and characteristics of geospatial networks go beyond standard graph topology, and therefore it is crucial to explicitly take space into account when analyzing them. The sfnetworks R package is created to facilitate such an integrated workflow. It brings together the sf package for spatial data science and the tidygraph package for standard graph analysis. The core of the package is a data structure that can be provided as input to both the graph analytical functions of tidygraph as well as the spatial analytical functions of sf, without the need for conversion. Additionally, it offers a set of geospatial network specific functions, such as routines for shortest path calculation, network cleaning and topology modification. The package is designed as a general-purpose package suitable for usage across different application domains, and can be seamlessly integrated in """"tidy"""" workflows that use the tidyverse packages for data science.",Spatial analysis,3B - Spatial Analysis
2021,autotest: Automatic testing of R packages,Regular talks > Track C,UseR! 2021,UseR! 2021,"Padgham, Mark",The Lounge #talk_packages_2,"The 'autotest' package has been developed by rOpenSci to automatically test the robustness of R packages to unexpected inputs. We hope that its usage will enable and encourage software to reach the highest possible quality prior to our peer-review process. 'autotest' implements a form of mutation testing which identifies expected or permitted forms for each parameter, and examines how each function responds to mutations of those inputs. Many software bugs are uncovered by packages being used in ways that developers themselves may not have anticipated, yet no developer can anticipate all potential ways software may be used. 'autotest' eases the task of making software robust to """"unexpected"""" usage by testing and reporting any points at which mutations to inputs generate unexpected results.

  
The package also matches expectations with textual descriptions provided by function documentation, and ensures that descriptions of input and output parameters are sufficient for users to understand ranges of admissible inputs, and of returned values. Application of 'autotest` to a package should thus ensure that forms and ranges of every parameter of every function are clearly described, and that all functions respond consistently to as many diverse forms of input as possible.

  
Finally, 'autotest' can also be used to automatically generate a package test suite. Although highly variable, applying the test to a package consisting primarily of numeric algorithms can """"automatically"""" generate a test suite covering well over 50% of code.","Efficient programming, R in production",3C - R Packages 2
2021,Triplot: model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure,Regular talks > Track A,UseR! 2021,UseR! 2021,"Pękala, Katarzyna",The Lounge #talk_ml_dm,"One of the key elements of the explanatory analysis of a predictive model is to assess the importance of the individual variables. The rapid development of the area of predictive model exploration (also called explainable artificial intelligence or interpretable machine learning) has led to the popularization of methods for local (instance level) and global (dataset level) methods, such as Permutational Variable Importance, Shapley Values (SHAP), Local Interpretable Model Explanations (LIME), Break Down and so on. However, these methods do not use information about the correlation between features which significantly reduce the explainability of the model behaviour.

  
In this work, we propose new methods to support model analysis by exploiting the information about the correlation between variables. The dataset level aspect importance measure is inspired by the block permutations procedure, while the instance level aspect importance measure is inspired by the LIME method. We show how to analyse groups of variables (aspects) both when they are proposed by the user and when they should be determined automatically based on the hierarchical structure of correlations between variables.

  
Additionally, we present a new type of model visualisation, triplot, that exploits a hierarchical structure of variable grouping to produce a high information density model visualisation. This visualisation provides a consistent illustration for either local or global model and data exploration.

  
We also show an example of real-world data with 5k instances and 37 features in which a significant correlation between variables affects the interpretation of the effect of variable importance.

  
The proposed method is, to our knowledge, the first to allow direct use of the correlation between variables in exploratory model analysis. Triplot package for R is developed under open source GPL-3 licence and is available on GitHub repository at <https://github.com/ModelOriented/triplot>.",Data mining / Machine learning / Deep Learning and AI,3A - Machine Learning and Data Management
2021,"slopes: a package for calculated slopes of roads, rivers and other linear (simple) features",Regular talks > Track B,UseR! 2021,UseR! 2021,"Lovelace, Robin; Félix, Rosa",The Lounge #talk_spatial_analysis,"The goal of the slopes is to enable reproducible calculation of slopes for urban, transport, ecological applications and research projects using free and open source software. We have developed the package to be fast, accurate and user friendly, calculating the longitudinal steepness of linear features such as roads and rivers based on open access datasets such as road geometries and digital elevation models (DEMs). The package has a few unique features, including the ability to calculate slopes based on multiple input classes for raster data, and the ability to download and use DEM data on-the-fly in places where users lack DEM data. The package is work in progress but has already attracted attention from road steepness maps of cities in Portugal and Brazil. Integrating with other packages such as sf and sfnetworks, the package should provide a strong foundation for research into the impacts of vertical gradient profiles on phenomena ranging from aquatic migration patterns to flooding and walking and cycling potential. In the talk we will present both the package and some of the research questions we have used it to explore and will ask the audience: how steep a hill would you be willing to walk or cycle up? We will conclude by discussing limitations of the package and future directions of development.",Spatial analysis,3B - Spatial Analysis
2021,A fresh look at unit testing with tinytest,Regular talks > Track C,UseR! 2021,UseR! 2021,"van der Loo, Mark",The Lounge #talk_packages_2,"""""The tinytest package\[1,2\] implements a light weight and flexible framework for unit testing R packages. In spite of its young age, tinytest has become quite popular: since it was released on CRAN in the spring of 2019, more than 140 packages on CRAN and Bioconductor have started to use tinytest for automatic unit testing. This includes influential packages such as Rcpp.

  
Tinytest has a few unique features that set it apart from other testing frameworks, such as parallelization and tracking of side-effects. Side effects such as changes in environment variables are important, for example when working with locale-sensitive operations such as sorting, or date-time conversions. Tinytest also makes it easy to temporarily manipulate the testing environment during the run of the test. For example by changing environment

  
variables. In tinytest, test results are just another type of data. They can be easily translated to data frame layout which to investigate results, or export them from an automated build environment. Moreover, tests are installed with the package so package authors can ask their users to run tests on the user's infrastructure. Using tinytest is easy, as test scripts require no special code: tinytest automatically collects and organizes test results that are

  
created by any unit test expectation occurring in the script.

  
As the name suggests, tinytest is a small package, built in less than 1200 lines of code and no dependencies other than two R-base packages that come with any R installation.

  
\[1\] M van der Loo (2017). tinytest: R package version 1.2.4. <https://cran.r-project.org/package=tinytest>

  
\[2\] MPJ van der Loo (2020) A method for deriving information from running R code. R-Journal (Accepted) <https://arxiv.org/abs/2002.07472>","Efficient programming, R in production",3C - R Packages 2
2021,Getting sprung in R: Introduction to the rsetse package for embedding feature-rich networks,Regular talks > Track A,UseR! 2021,UseR! 2021,"Bourne, Jonathan",The Lounge #talk_ml_dm,"The Strain Elevation Tension Spring embedding algorithm (SETSe) is a deterministic method for embedding feature-rich networks. The algorithm uses simple Newtonian equations of motion and Hooke's law to embed the network onto a locally euclidean manifold. To create the embedding, SETSe converts node attributes into forces and the edge attributes into springs. SETSe finds an equilibrium position when the forces on the springs balance the forces of the nodes. The algorithm has a time complexity of O(2) and linear memory complexity; this means the algorithm avoids issues faced by other physics based embedding methods and can be used to embed graphs with tens of thousands of nodes and more than a million edges.

  
Some applications of SETSe are; analysing social networks; understanding the robustness of power grids; geographical analysis; predicting node features; understanding power dynamic between individuals and organisations; analysis of molecular structures.

  
This presentation will provide both a brief technical discussion of the algorithm and its implementation, as well as several use cases. The use cases describe how to embed a network and then how to interpret that embedding.

  
There are very few options for graph embeddings using R, and this is something that rsetse seeks to address; the algorithm has been implemented in the package rsetse and is available on CRAN.",Data mining / Machine learning / Deep Learning and AI,3A - Machine Learning and Data Management
2021,"mapsf, a New Package for Thematic Mapping",Regular talks > Track B,UseR! 2021,UseR! 2021,"Giraud, Timothée",The Lounge #talk_spatial_analysis,"{mapsf} helps to design various cartographic representations such as proportional symbols, choropleth or typology maps. It also offers several functions to display layout elements that improve the graphic presentation of maps.

  
The aim of {mapsf} is to obtain thematic maps with the visual quality of those built with a classical mapping or GIS software while being lightweight, versatile and user-friendly. To achieve this goal, the package takes advantage of the features offered by {sf} and provides a limited number of simple mapping functions.

  
{mapsf} is the successor of {cartography}, it offers the same core features but it is simpler and more robust. Unlike other popular cartographic packages, it does not use grammar of graphics, it depends on a limited number of packages and displays georeferenced plots using base R graphics.

  
The main function of the package, mf\_map(), gives access to 9 map types: base maps, proportional or graduated symbols, choropleth maps, typology maps and various combinations of symbology. Many parameters are available to fine tune the cartographic representations. These parameters are the common ones found in GIS and automatic cartography tools (e.g. classification, color palettes, symbols sizes, legend layout...).

  
Some additional functions are dedicated to layout design (graphic themes, legends, scale bar, north arrow, title, credits…), map insets or map exports.

  
The development of {mapsf} follows the current best practices of the R ecosystem (CI/CD, coverage tests) and its documentation is enhanced by a vignette and a website.",Spatial analysis,3B - Spatial Analysis
2021,{fusen}: Create a package from Rmd files,Regular talks > Track C,UseR! 2021,UseR! 2021,"Rochette, Sébastien",The Lounge #talk_packages_2,"You know how to build a Rmarkdown for reproducibility, you were said or would like to put your work in a R package, but you think this is too much work? You do not understand where to put what and when? What if writing a Rmd was the same as writing a package? Let {fusen} help you with this task.

  
When you write a Rmarkdown file (or a vignette), you create a documentation for your analysis (or package). Inside, you write some functions, you apply your functions to examples and you maybe write some unit tests to verify the outputs. This is even more true if you follow this guide : ['Rmd first': When development starts with documentation](https://rtask.thinkr.fr/blog/rmd-first-when-development-starts-with-documentation/).

  
Why not transforming this workflow into a documented, tested and maintainable R package to ensure sustainability of your analyses ? To do so, you will need to move your functions and scripts in the correct place. Let {fusen} do this transformation for you!

  
{fusen} is first addressed to people who never wrote a package before but know how to write a RMarkdown file. Understanding package infrastructure and correctly settling it can be frightening. This package may help them do the first step!

  
{fusen} is also addressed to more advanced developers who are fed up with switching between R files, tests files, vignettes. In particular, when changing arguments of a function, we need to change examples, unit tests in multiple places. Here, you can do it in one place. No risk to forget one. Package {fusen} is himself built with himself, from Rmd template files stored in the appropriate place","Efficient programming, R in production",3C - R Packages 2
2021,An R package for the implementation of Efficiency Analysis Trees and the estimation of technical efficiency,Regular talks > Track A,UseR! 2021,UseR! 2021,"Esteve, Miriam",The Lounge #talk_ml_dm,"EAT is a new R package that includes functions to estimate production frontiers and technical efficiency measures using non-parametric techniques based on CART regression trees. The package implements the main algorithms associated with a new technique introduced to estimate the efficiency of a set of decision making units in Economics and Engineering through machine learning techniques, called Efficiency Analysis Trees (Esteve et al., 2020). It encompasses the estimation of radial measures, oriented Russell efficiency measures, the directional distance function, the weighted additive model, graphical representations of the production frontier using tree-shaped structures and the classification of input variable importance. In addition, it incorporates a code to carry out an adaptation of the Random Forest Algorithm to estimate technical efficiency. This work describes the methodology and application of the functions.",Data mining / Machine learning / Deep Learning and AI,3A - Machine Learning and Data Management
2021,"osmextract: An R package to download, convert, and import large OpenStreetMap datasets",Regular talks > Track B,UseR! 2021,UseR! 2021,"Gilardi, Andrea",The Lounge #talk_spatial_analysis,"OpenStreetMap (OSM) is an online database that provides open-access geographic and rich attribute data worldwide, representing a wide range of physical and human features, including roads, rivers, and political boundaries. OSM is the world’s largest open-access source of geographic vector data, comprising nodes (points), ways (lines and polygons) and relations (describing a wide range of entities). Practical applications include disaster response, transport planning, and service location. OSM datasets can be manually downloaded from the project’s servers directly or via the R package osmdata, which uses the Overpass API. Large 'extracts' are also available from external providers (such as geofabrik.de) in a compressed binary format based on protocol buffers. The aim of osmextract is to enable processing and import of such OSM extracts. The package is composed of three main functions that can be used to 1) match an input location with one of the OSM extracts, either via spatial matching or approximate string distance; 2) download the chosen file; 3) convert the compressed data to Geopackage format. The main function, named oe\_get(), returns sf objects. This workflow is effective for importing OSM extracts covering large geographical areas. Furthermore, the conversion process is based on GDAL routines, enabling customized spatial filters or SQL-like queries, further boosting import performance.",Spatial analysis,3B - Spatial Analysis
2021,Questions,Regular talks > Track C,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_packages_2,Last questions for all the speakers,Community and Outreach,3C - R Packages 2
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_ml_dm,Last questions for all the speakers,Community and Outreach,3A - Machine Learning and Data Management
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_spatial_analysis,Last questions for all the speakers,Community and Outreach,3B - Spatial Analysis
2021,Let Me Google That for You – Measuring global trends using Google Trends,Regular talks > Track A,UseR! 2021,UseR! 2021,"Puhr, Harald",The Lounge #talk_trends_markets_models,"We present the globaltrends package as a flexible and user-friendly means to analyze data from Google Trends. Google offers public access to global search volumes from its search engine through the Google Trends portal. Users select keywords for which they want to obtain search volumes and specify time period and location (global, country, state) of interest. For these combinations of keywords, periods, and locations Google Trends provides search volumes that indicate the number of search queries submitted to the Google search engine. However, Google constrains users to batches of five keywords and normalizes results for each batch. Thereby, large-scale analysis and comparability across batches is impaired. By re-normalizing results to a set of user-defined baseline keywords, the globaltrends package overcomes these limitations. This gives users the opportunity to download and measure search scores i.e., volumes set to a common baseline, for several keywords across or within locations. In addition, users can visualize distributions, developments, and out-of-the-ordinary changes in global search scores or for specific locations. The package allows researchers and analysts to use these search scores to investigate global trends based on patterns within them. This offers insights such as degree of internationalization of firms and organizations or dissemination of political, social, or technological trends across the globe or within single countries.","Economics / Finance / Insurance, Social sciences","4A - Trends, Markets and Models"
2021,Visual Diagnostics for Constrained Optimisation with Application to Guided Tours,Regular talks > Track B,UseR! 2021,UseR! 2021,"Zhang, H. Sherry",The Lounge #talk_dataviz_spatial,"Guided tour searches for interesting low-dimensional views of high-dimensional data via optimising a projection pursuit index function. The first paper of projection pursuit by Friedman and Tukey (1974) stated that “the technique used for maximising the projection index strongly influences both the statistical and the computational aspects of the procedure.” While much work has been done in proposing indices in the literature, less has been done on evaluating the performance of the optimisers. In this paper, we implement a data collection object in the optimisation of projection pursuit guided tour and introduce visual diagnostics based on the data object collected. These diagnostics and this workflow can be applied to a broad class of optimisers, to assess their performance. An R package, ferrn, has been created to implement the diagnostics.","Data visualisation, Spatial analysis",4B - Data viz and Spatial Applications
2021,Computing Disposition Effect on Financial Market Data,Regular talks > Track A,UseR! 2021,UseR! 2021,"Mazzucchelli, Lorenzo; Zanotti, Marco",The Lounge #talk_trends_markets_models,"In recent years, an irrational phenomenon in financial markets is grabbing the attention of behavioral economists: the disposition effect. Firstly discovered by H. Shefrin and M. Statman (1985), the disposition effect consists in the realization that investors are more likely to sell an asset when it is gaining value compared to when it is losing value. A phenomenon which is closely related to sunk costs’ bias, diminishing sensitivity, and loss aversion.

  
From 1985 until now, the disposition effect has been documented in US retail stock investors as well as in foreign retail investors and even among professionals and institutions. By the time, it is a well-established fact that the disposition effect is a real behavioral anomaly that strongly influences the final profits (or losses) of investors. Furthermore, being able to correctly capture these irrational behaviors timely is even more important in periods of high financial volatility as nowadays.

  
The presentation focuses on the new dispositionEffect R package that allows to quickly evaluate the presence of disposition effect’s behaviors of an investor based solely on his transactions and the market prices of the traded assets. A simple step-by-step practical guide is presented to understand how to effectively use all the implemented functionalities. Finally, since financial data may be potentially huge in size, efficiency concerns are discussed and the parallelized versions of the functions are shown.","Economics / Finance / Insurance, Social sciences","4A - Trends, Markets and Models"
2021,The R Package diseq: Estimation Methods for Markets in Equilibrium and Disequilibrium,Regular talks > Track A,UseR! 2021,UseR! 2021,"Karapanagiotis, Pantelis",The Lounge #talk_trends_markets_models,"Market models constitute a major cornerstone of empirical research in industrial organization and macroeconomics. Previous literature in these fields has proposed a variety of estimation methods both for markets in equilibrium, which typically entail a market-clearing condition, and in disequilibrium, in which the primary identification condition comes from the short-side rule. Although methodologically attractive, the estimation methods of such models, in particular of the disequilibrium models, is computationally demanding and software providing simple, out-of-the-box methods for estimating them is scarce. Econometricians, therefore, mostly rely on their own implementations for estimating these models. This talk presents the R package Diseq, which provides functionality to simplify the estimation of models for markets in equilibrium and disequilibrium using full information maximum likelihood methods. The basic functionality of the package is presented based on the data and the classic analysis originally performed by Fair &amp; Jaffee (1972). The talk also gives an overview of the design of the package, presents the post-estimation analysis capabilities that accompany it, and provides statistical evidence of the computational performance of its functionality gathered via large-scale benchmarking simulations. Diseq is free software that is distributed under the MIT license as part of the R software project. It comprises a set of estimation tools, which are to a large extend not available from either alternative R packages or other statistical software projects.","Economics / Finance / Insurance, Social sciences","4A - Trends, Markets and Models"
2021,geofi-package: Facilitating the access to key spatial datasets in Finland,Regular talks > Track B,UseR! 2021,UseR! 2021,"Kainu, Markus",The Lounge #talk_dataviz_spatial,"There is a groving demand for presenting statistical data on map. COVID-19 launched a race across internet in spatial data visualization where aesthetics, usability and real-timeliness are of high value. The demand for real-time data favours solutions that can be scripted, automated and refactored quickly and for that purpose we developed geofi R-package to facilitate access to key Finnish geospatial datasets. <https://ropengov.github.io/geofi/> geofi combines resources of two Statistics Finland APIs: the regional classification API and spatial data API. Time-series of regional classifications are shipped as on-board data and larger spatial data is fetched through a WFS-api consisting of administrative borders, zipcodes and both population and statistical grids at various resolutions. Package aims at being a onboarding technology into R ecosystem with clear and concrete vignettes covering the basics of spatial data manipulation, working with attribute data and step-by-step instruction for creating maps both for static and interactive applications. This talk describes the main functions and design principles of the packages and makes comparison with similar packages such as geobr for Brazil and geouy for Uruguay.","Data visualisation, Spatial analysis",4B - Data viz and Spatial Applications
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_trends_markets_models,Last questions for all the speakers,Community and Outreach,"4A - Trends, Markets and Models"
2021,Virtual Environments: Using R as a Frontend for 3D Rendering of Digital Landscapes,Regular talks > Track B,UseR! 2021,UseR! 2021,"Mahoney, Michael J.",The Lounge #talk_dataviz_spatial,"This talk discusses a new approach to using R to create 3D landscape visualizations, which relies on external tooling designed specifically for detailed 3D rendering and interactive exploration. By using R as a frontend for high-performance rendering engines, users are able to quickly create data-defined renders which can then be interactively explored and manipulated. Two of the most promising engines for this approach are the (proprietary, source-available) Unity rendering engine, which excels at visualizing large swaths of land and the (free and open-source) Blender engine, which is well-adapted for visualizing smaller settings.

  
Our new {terrainr} package (available from CRAN) helps users quickly produce terrain surfaces from real-world data in Unity, visualizing environmental patterns and processes across large scales. Two new experimental packages, {mvdf} and {forthetrees}, focus on creating smaller-scale renders in Blender. Taken together, these packages suggest a way for users to create data-defined 3D renderings within R, using their preexisting coding abilities in the place of complex user interfaces to control powerful rendering engines. Our approach makes it possible for users to create renderings from data in these engines faster and easier than has been historically possible.","Data visualisation, Spatial analysis",4B - Data viz and Spatial Applications
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_dataviz_spatial,Last questions for all the speakers,Community and Outreach,4B - Data viz and Spatial Applications
2021,Elevator Pitches 2,Elevator Pitches,UseR! 2021,UseR! 2021,,The Lounge #elevator_pitches,"Elevator pitches (short videos and technical notes):

1. Make Your Computational Analysis Citable (Batool Almarzouq)
2. Structural connectivity in the Lower Uruguay River Forest (Adriana Rojas, Mariel Bazzalo, Natalia Morandeira)
3. Fitting the beta distribution for the intra-apiary dynamics study of the infestation rate with Varroa Destructor in honey bee colonies (Camila Miotti, Ana Molineri, Adriana Pacini, Emanuel Orellano, Marcelo Signorini, Agostina Giacobino)
4. C50 Classification of young Moroccan men and women not in employment, education or training (NEET). (Salima MANSOURI, Hafsa EL HAFYANI, Ichark LAFRAM)
5. Decision support using R and DataOps at a European Union bank regulator (Jonas Bergstrom, Nicolas Pochet)
6. fipp : a bridge between domain knowledge and model specification in Dirichlet Process Mixtures and Mixture of Finite Mixtures (Jan Greve, Bettina Grün, Gertraud Malsiner-Walli, Sylvia Frühwirth-Schnatter)
7. An integrated teaching environment for R with {learnitdown} (Philippe Grosjean, Guyliann Engels)
8. Visualization of the one-way flexible ANOVA tests using with {doexplot} (Mustafa Cavus)
9. TrackJR: a new R-package using Julia language for tracking tiny insects (Gerardo de la vega, Federico Triñanes, Andres Gonzalez Ritzel)
10. shiny.fluent and shiny.react: Build Beautiful Shiny Apps Using Microsoft's Fluent UI (Marek Rogala)
11. Conducting Effective User Tests for Shiny Dashboards (Maria Grycuk)
12. NNcompare: An R package supporting the peer programming process in clinical studies (Mette Bendtsen, Steffen Falgreen Larsen, Frederik Vandvig Heinen, Claus Dethlefsen)
13. The evolution of the dependencies of CRAN packages (Clement Lee)
14. Bootstrapping Multilevel Models in R using lmeresampler (Adam Loy)
15. RCode, a new IDE for R (Nicolas Baradel, William Jouot)
16. R Package Validation and {valtools} (Ellis Hughes)
17. NetCoupler: Inferring causal pathways between high-dimensional metabolomics data and external factors (Luke W. Johnston, Clemens Wittenbecher, Fabian Eichelmann)
18. Continuously expanding Techguides: An open source project based on bookdown using CI/CD pipelines from GitHub Actions (Peter Schmid)
19. R as a an environment for the functional analysis of proteins (Michał Burdukiewicz)
20. Statistical Workflows in R for Imaging Mass Spectrometry Data (Hoang Tran, Valeriia Sherina, Fang Xie)
21. Hi, Let’s Talk About Data Science! - Customize Your Personal Data Science Assistant Bot. (Livia Eichenberger, Oliver Guggenbühl)
22. NNSampleSize: A tool for communicating, determining and documenting sample size in clinical trials (Claus Dethlefsen, Steffen Falgreen Larsen, Anders Ellern Bilgrau, Nynne Holdt-Caspersen, Maika Lindkvist Jensen)
23. pARI package: valid double-dipping via permutation-based All Resolutions Inference (Angela Andreella, Jelle Goeman, Livio Finos, Wouter Weeda, Jesse Hemerik)
24. Data Access and dynamic Visualization for Clinical Insights (DaVinci) (Matthias Trampisch, Julia Igel, Andre Haugg)
25. Reproducibility and dissemination in the research: a case of study of the bioaerosol dynamics (Jesús Rojo, Antonio Picornell, Jeroen Buters, Jose Oteros)
26. R in the aiR! (Adithi R. Upadhya, Pratyush Agrawal, Sreekanth Vakacherla, Meenakshi Kushwaha)
27. segmenter: A Wrapper for JAVA ChromHMM (Mahmoud Ahmed, Deok Ryong Kim)
28. Efficient list recursion in R with rrapply (Joris Chau)
29. An R package to flexibly generate simulation model flow diagrams (Andreas Handel, Andrew Tredennick)
30. Using R Markdown to Automate COVID-19 Reporting (Farzad Islam, Michael Elten, Najmus Saqib)
31. Cumulative Link Mixed-effects Models (CLMMs) as a tool to model ordinal response variables and incorporate random effects (Christophe Bousquet)
32. High dimensional data visualization in ggplot2 (Zehao Xu, Wayne Oldford)
33. Charting Covid with the DatawRappr-Package (Benedict Witzenberger)
34. Bringing AutoDiff to R packages (Michael Komodromos)
35. Healthier &amp; Happier Hands: Software and Hardware Solutions for More Ergonomic Typing (John Paul Helveston)
36. High Dimensional Penalized Generalized Linear Mixed Models: The glmmPen R Package (Hillary M. Heiling, Naim U. Rashid, Quefeng Li, Joseph G. Ibrahim)
37. trackdown: collaborative writing and editing your R Markdown and Sweave documents in Google Drive (Filippo Gambarota, Claudio Zandonella Callegher, Janosch Linkersdörfer, Mathew Ling, Emily Kothe)
38. Multivariate functional data analysis (Manuel Oviedo-de la Fuente, Manuel Febrero-Bande)
39. Multivariate functional principal component analysis on high dimensional gait data (Sajal Kaur Minhas, Morgan Sangeux, Julia Polak, Michelle Carey)
40. Teaching an introductory programming course with R (Reto Stauffer, Joanna Chimiak-Opoka, Luis M Rodriguez-R, Achim Zeileis)
41. Landscape of R packages for eXplainable Artificial Intelligence (Szymon Maksymiuk, Alicja Gosiewska, Przemysław Biecek)
42. Reactive PK/PD: An R shiny application simplifying the PK/PD review process (Kristoffer Segerstrøm Mørk, Steffen Falgreen Larsen)
43. r-cubed: Guiding the overwhelmed scientist from random wrangling to Reproducible Research in R (Hannah Chatwin, Luke W. Johnston, Helene Baek Juel, Bettina Lengger, Daniel R. Witte, Malene Revsbech Christiansen, Anders Aasted Isaksen)
44. Validate observations stored in a DB (Edwin de Jonge)
45. Teaching Biology students to code smoothly with learnR and gradethis (Guyliann Engels, Philippe Grosjean)
46. Partial Least Squares Regression for Beta Regression Models (Frederic Bertrand, Myriam Maumy)
47. Simpler is Better: Lifting Interpretability-Performance Trade-off via Automated Feature Engineering (Alicja Gosiewska, Anna Kozak, Przemysław Biecek)
48. State of the Market - Infinite State Hidden Markov Models (Dean Markwick)
49. networkABC: Network Reverse Engineering with Approximate Bayesian Computation (Myriam Maumy, Frederic Bertrand)
50. Navigating Insurance Claim Data Through Tidymodels Universe (Lok, Jun Haur; Kam, Tin Seong)",EP,EP2
2021,Panel: R User or R Developer? This is the question!,Panels and incubators,UseR! 2021,UseR! 2021,"Vitalini, Francesca",The Lounge #panel_user_developer,"Since its first official release back in 1995, R has outgrown its statistician-tool origin, spreading out to different fields. A key factor in R’s popularity is without a doubt its approachability for people without a software engineering background.

  
As a result, R is often considered more as a scripting / prototyping / data analytics tool than a proper software development language. Thanks to its low barrier and accessibility, however, people with a wide variety of (non-technical) backgrounds can quickly become active and effective users. This in turn can set the ground for exploring and building up programming and development skills, transitioning towards what is normally associated with software engineering profiles.

  
This raises some questions: what does it mean to be an R user? Is there such a thing as an R developer, and (how) does it differ from being an R user? In this time where IT skills are required across virtually every domain, can an R user afford not to be a software engineer as well? What is (and does it even exist) the R equivalent of the Python stack developer? What type of background and expertise should an R user have to fit what companies are looking for? And what about academia? What is the current trend?

  
We will discuss these questions in a panel featuring the point of view of:

  
- 
- experts from both industry and academia,
- data scientists who have made the transition from R users to software developers,
- R Core team,

  
and of course the Community perspective.",Other,Panel: R User or R Developer? This is the question!
2021,Incubator: The role of the R community in the RSE movement,Panels and incubators,UseR! 2021,UseR! 2021,"Seibold, Heidi; Turner, Heather; Bannert, Matt",The Lounge #incubator_rse,"The term """"Research Software Engineer""""(RSE) was proposed by a group of software developers working in academia at a workshop in Oxford, UK, 2012. It was the beginning of a grass-roots movement to establish Research Software Engineering as a profession for people that combined expertise in programming with an intricate understanding of research. Since then, the movement has grown substantially, leading to recognition, reward and career opportunities for RSEs and the creation of national RSE associations in Australia/New Zealand, Belgium, Germany, the Netherlands, the Nordic region, the UK and the USA.

  
This incubator will provide an opportunity to discuss the role of the R community in the RSE movement. What can we share with this wider community? How can we help the movement grow? What could the R community gain from this movement? We will identify a range of actions from quick wins to more ambitious projects that could be pursued after useR! 2021.",Community and Outreach,Incubator: The role of the R community in the RSE movement
2021,Developing a datasets based R package to teach environmental data science,Regular talks > Track A,UseR! 2021,UseR! 2021,"Horst, Allison",The Lounge #talk_teaching,"There are many openly available environmental datasets out there. However, it is time and energy consuming for teachers to identify, explore and clean complex datasets for use in environmental data science classes. As the success (&gt;60k downloads) of the recent palmerpenguins R package demonstrates, there is strong demand and interest in curated real-world datasets ready to be used “out of the box” for data science teaching purposes. In this project, our goal was to develop a sample dataset and an associated analytical example for every site of the Long Term Ecological Research (LTER) network. This network, founded by the US National Science Foundation, is made of 30 sites where both observational and experimental environmental data sets are collected with a long-term perspective, and thus provide a treasure trove of interesting, real-world environmental data. All of those resources have been combined into one R package. R packages are an ideal vehicle for teaching datasets because R is widely used in environmental research communities and degree programs, and packages can be installed in one command. In addition, the R Markdown ecosystem provides a suite of tools to publish the documentation and examples as a website to expose all the pedagogic content to non-R users as well. We relied on the package structure to develop a reproducible workflow to ingest and document the LTER data. We also wanted to share the code necessary to access the full dataset to enable further investigation of more complex datasets. In this presentation, we will explain our process to design this R package and provide a set of analysis examples for environmental data science teaching purposes.",Teaching R/R in Teaching,5A - Teaching R and R in Teaching
2021,maars: Tidy Inference under misspecified statistical models in R,Regular talks > Track B,UseR! 2021,UseR! 2021,"Fogliato, Riccardo; Shrotriya, Shamindra",The Lounge #talk_math_stats,"Linear regression using ordinary least squares (OLS) is a critical part of every statistician's toolkit. In R, this is elegantly implemented via lm() and its related functions. However, the statistical inference output from this suite of functions is based on the assumption that the model is well specified. This assumption is often unrealistic and at best satisfied approximately. In the statistics and econometric literature, this has long been recognized and a large body of work provides inference for OLS under more practical assumptions (e.g., only assuming independence of the observations). In this talk, we will introduce our package “maars” (models as approximations) that aims at bringing research on inference in misspecified models to R via a comprehensive workflow. Our """"maars"""" package differs from other packages that also implement variance estimation, such as “sandwich”, in three key ways. First, all functions in “maars” follow a consistent grammar and return output in tidy format (Wickham, 2014), with minimal deviation from the typical lm() workflow. Second, “maars'' contains several tools for inference including empirical, wild, residual bootstrap, and subsampling. Third, “maars” is developed with pedagogy in mind. For this, most of its functions explicitly return the assumptions under which the output is valid. This key innovation makes “maars” useful in teaching inference under misspecification and also a powerful tool for applied researchers. We hope our default feature of explicitly presenting assumptions will become a de facto standard for most statistical modeling in R.","Data mining / Machine learning / Deep Learning and AI, Operational research and optimization, Statistical models",5B - Mathematical/Statistical Methods
2021,Using R as a Community Workbench for The Carpentries Lesson Infrastructure,Regular talks > Track A,UseR! 2021,UseR! 2021,"Kamvar, Zhian N.",The Lounge #talk_teaching,"The Carpentries is a global community of volunteers that collaboratively develops and delivers lessons to build capacity in data and coding skills (in R and multiple other languages) to researchers worldwide. For the past five years, our collaboratively-developed lesson template (<https://github.com/carpentries/styles/>) has been the basis for our growing collection of peer-reviewed lesson content. This template was fully self-contained with all the tools and styles needed to create a full lesson website. While the lessons themselves were designed to be easy to author, there were two significant barriers in our toolchain for contributors: software installation and style updating. As our lesson repertoire and community has continued to grow, this template model has not scaled well, resulting in barriers to entry and wasted volunteer time. In 2020 we began the process to redesign our template from the ground up using a combination of R’s literate programming ecosystem and GitHub Workflows, resulting in three R packages called {sandpaper}, {pegboard}, and {varnish} for handling, validating, and styling lessons. The new approach separates the content from the tools and style, allowing for seamless updates so the maintainers can focus on authoring their lessons and not on the tools needed to build them. To accommodate the wide array of diverse skill sets in our community, we wanted to ensure the tools could be used by anyone without any prior knowledge of R. We will detail how we involved our community in iterated development of the new template with user stories, passive community feedback, community member interviews, and user experience testing. In the end, we will show how the wide array of tools available in the R ecosystem makes it easy for us to rebuild our lesson infrastructure in a way that significantly reduces the barrier for entry for our community volunteers.",Teaching R/R in Teaching,5A - Teaching R and R in Teaching
2021,"Mixed Integer Evolutionary Strategies with \""\""miesmuschel\""\""",Regular talks > Track B,UseR! 2021,UseR! 2021,"Binder, Martin",The Lounge #talk_math_stats,"Evolutionary Strategies (ES) are optimization algorithms inspired by biological evolution that do not make use of gradient information, and are therefore well-suited for """"black-box optimization"""" where this information is not available. Mixed-Integer ES (MIES) are an extension that allow optimization of mixed continuous, integer, and categorical search spaces by defining different mutation and recombination operations on different subspaces. We present our new package """"miesmuschel"""" (pronounced MEES-mooshl), a modular toolbox for MIES optimization. It provides """"Operator"""" objects for mutation, recombination, and parent/survival selection that can be configured and combined in various ways to match the optimization problem at hand. Configuration parameters of operators can even be self-adaptive and evolve together with the solutions of the optimization problem. Miesmuschel can be used for both single- and multi-objective optimization, simply by using different selection operations. The multi-fidelity optimization capabilities of miesmuschel can be used for expensive objectives where early generations or new samples are preliminarily evaluated with less effort.

  
A standard optimization loop (parent selection, recombination, mutation, survival selection) is given and can be used out-of-the-box, but the supplied methods can also be combined as building blocks to form more specialized algorithms.

  
Miesmuschel makes use of the """"paradox"""" and """"bbotk"""" packages and integrates with the """"mlr3"""" ecosystem.","Data mining / Machine learning / Deep Learning and AI, Operational research and optimization, Statistical models",5B - Mathematical/Statistical Methods
2021,Teaching and Learning Bayesian Statistics with {bayesrules},Regular talks > Track A,UseR! 2021,UseR! 2021,"Dogucu, Mine",The Lounge #talk_teaching,"Bayesian statistics is becoming more popular in data science. Data scientists are often not trained in Bayesian statistics and if they are, it is usually part of their graduate training. During this talk, we will introduce an introductory course in Bayesian statistics for learners at the undergraduate level and comparably trained practitioners. We will share tools for teaching (and learning) the first course in Bayesian statistics, specifically the {bayesrules} package that accompanies the open-access Bayes Rules! An Introduction to Bayesian Modeling with R book. We will provide an outline of the curriculum and examples for novice learners and their instructors.",Teaching R/R in Teaching,5A - Teaching R and R in Teaching
2021,Here is the anomolow-down!,Regular talks > Track B,UseR! 2021,UseR! 2021,"Kandanaarachchi, Sevvandi",The Lounge #talk_math_stats,"Why should we care about anomalies? They demand our attention because they are telling a different story from the norm. An anomaly might signify a failing heart rate of a patient, a fraudulent credit card activity, or an early indication of a tsunami. As such, it is extremely important to detect anomalies.

  
What are the challenges in anomaly detection? As with many machine/statistical learning tasks high dimensional data poses a problem. Another challenge is selecting appropriate parameters. Yet another challenge is high false positive rates.

  
In this talk we introduce two R packages – dobin and lookout – that address different challenges in anomaly detection. Dobin is a dimension reduction technique especially catered to anomaly detection. So, dobin is somewhat similar PCA; but dobin puts anomalies in the forefront. We can use dobin as a pre-processing step and find anomalies using fewer dimensions.

  
On the other hand, lookout is an anomaly detection method that uses kernel density estimates and extreme value theory. But there is a difference. Generally, anomaly detection methods that use kernel density estimates require a user-defined bandwidth parameter. But does the user know how to specify this elusive bandwidth parameter? Lookout addresses this challenge by constructing an appropriate bandwidth for anomaly detection using topological data analysis, so the user doesn’t need to specify a bandwidth parameter. Furthermore, lookout has a low false positive rate because it uses extreme value theory.

  
We also introduce the concept of anomaly persistence, which explores the birth and death of anomalies as the bandwidth changes. If a data point is identified as an anomaly for a large range of bandwidth values, then its significance as an anomaly is high.","Data mining / Machine learning / Deep Learning and AI, Operational research and optimization, Statistical models",5B - Mathematical/Statistical Methods
2021,Building and maintaining OpenIntro using the R ecosystem,Regular talks > Track A,UseR! 2021,UseR! 2021,"Cetinkaya-Rundel, Mine",The Lounge #talk_teaching,"OpenIntro's (openintro.org) mission is to make educational products that are free and transparent and that lower barriers to education. The products include textbooks (in print and online), supporting resources for instructors as well as for students. From day one, OpenIntro materials have been built using tools within the R ecosystem. In this talk we will discuss how the OpenIntro project has shaped and grown over the years, our process for developing and publishing open-source textbooks at the high school and college level, and our computing resources such as interactive R tutorials and R packages as well as labs in various languages. We will highlight recent workflows we have developed and lessons learned for converting books from LaTeX to bookdown and give an overview of our project organization and tooling for authoring, collaboration, and maintenance, much of which is built with R, R Markdown, Git, and GitHub. Finally, we will discuss opportunities for getting involved for educators and students contributing to the development of open-source educational resources under the OpenIntro umbrella and beyond.",Teaching R/R in Teaching,5A - Teaching R and R in Teaching
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_math_stats,Last questions for all the speakers,Community and Outreach,5B - Mathematical/Statistical Methods
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_teaching,Last questions for all the speakers,Community and Outreach,5A - Teaching R and R in Teaching
2021,Incubator: Strategies to build a strong AsiaR Community,Panels and incubators,UseR! 2021,UseR! 2021,"Upadhya, Adithi R.; Ravi, Dr. Janani",The Lounge #panel_asian,"R has been a very inclusive community and collective learning has always helped, with many users of R in Asian countries we can as well have a strongly knit community for R’s users. Inspired by the MENA (Middle East and North Africa) R, AfricaR, and LatinR user groups, we propose a similar panel discussion to connect and strengthen the R community in Asia. We aim to target participants who are active R users and/or learners who have not been engaged with any R community and we want to invite panelists who are successful R developers/educators/community leaders in various Asian countries. Our goal is to build a diverse and vibrant R community within Asia. We wish to connect Asian useRs to each other, identify Asian R speakers/participants and facilitate regular webinars, workshops. We want to address the lower participation of Asian, specially Asian Underrepresented Minorities in local meetups and international conferences like useR! 2021, discuss and learn about best practices for nucleating and sustaining an engaged community. We also would like to understand how people from various backgrounds and organisations engage the community for assistance. We also wish to build a strong enough community to host a AsiaR conference in the upcoming years.",Community and Outreach,Panel: Strategies to build a strong AsiaR Community
2021,Easy R Markdown reporting with chronicle,Regular talks > Track A,UseR! 2021,UseR! 2021,"Heymans Smith, Philippe",The Lounge #talk_dataviz,"The chronicle package aims to ease the process of making R Markdown reports for R practitioners. With chronicle, the user is only required to provide the data and structure of the report, and chronicle will write the corresponding R Markdown file on behalf of the user. This means that the user can take the role of a director of the report, focusing on its content and structure, while delegating all the intricacies of visual consistency and interactiveness to the package.

  
chronicle currently supports 16 of the most popular R Markdown output formats, and lets the user add each element of a report in an additive paradigm inspired by ggplot.",Data visualisation,6A - Data visualisation
2021,Sponsored Talk by MemVerge,Regular talks > Track B,UseR! 2021,UseR! 2021,"Berry, Frank",The Lounge #talk_r_production_1,TBA,R in production,6B - R in Production 1
2021,virgo: a layered interactive grammar of graphics in R,Regular talks > Track A,UseR! 2021,UseR! 2021,"Lee, Stuart",The Lounge #talk_dataviz,"The virgo package enables interactive graphics for exploratory data analysis (EDA). Like ggplot2, our package takes a grammar based approach, that is, variables are mapped to visual encodings and plots are built layer by layer with marks. However, unlike ggplot2, the virgo package incorporates interactivity directly into its design by extending the Vega-Lite Javascript library and the vegawidget R package.

  
Users can easily initialize """"selection"""" objects to specify client side events like brushing or clicking. Once a """"selection"""" object is specified it can used in two different ways. First, a """"selection"""" can be broadcast to modify an encoding channel - for example, points being colored after a selection event has happened. Second, the data in a visual layer can react to a """"selection"""" - for example, computing a mean on the fly given the occurence of a selection event. Through composing multiple selection objects we can achieve rich interactivity.

  
In this talk, we will discuss the motivations behind the virgo package and grammar. We will demonstrate how virgo seamlessly integrates into existing EDA workflows through a case study. The virgo package is available online at <https://vegawidget.github.io/virgo>.",Data visualisation,6A - Data visualisation
2021,Bridging the Unproductive Valley: Building Data Products Strictly Without Magic,Regular talks > Track B,UseR! 2021,UseR! 2021,"Held, Maximilian",The Lounge #talk_r_production_1,"Between GUI-based reports and scripted data science lies an unproductive valley that combines the worst of both worlds:

  
poor scaleability *and* high overhead.

  
To avoid getting stuck there, small and medium-sized teams must 1) build strategic data products (not one-off scripts), 2) adopt software development best practices (not hacks) and 3) concentrate on business value (not infrastructure).

  
1\) Strategic data products focus on the ETL pipelines, common visualisations and other modules that are central to the mission. These unix-style building blocks can then be recombined into various reports.

  
2\) These modules are designed """"as-if-for-CRAN"""" and written as type/length-stable, unit-tested and exported functions.

  
3\) If something is not related to our mission, we rely on industry standards (Docker) and CaaS/DBaaS (Azure, GCP).

  
{muggle}'s opininated DevOps provides some technical scaffolding to help with this transition.

  
It standardises the compute environment in development, testing and deployment on a multi-stage Dockerfile with ONBUILD triggers for lightweight target images and leverages public cloud services (RSPM, GitHub Actions, GitHub Packages).

  
In contrast to some existing approaches, {muggle} never infers developer intent and has a minimal git footprint.

  
Success also requires a cultural shift. Development may still be agile, but it must not build prototype code. Fancy plots and reports are good, but reproducibility is more important.

  
We believe this is a necessary change to ensure value generation, and thereby, to ensure the future of democratic, and open-source data science.",R in production,6B - R in Production 1
2021,New displays for the visualization of multivariate data in the tourr package,Regular talks > Track A,UseR! 2021,UseR! 2021,"Laa, Ursula",The Lounge #talk_dataviz,"Tour methods allow the visualization of multi-dimensional structures as animated sequences of interpolated projections. The viewer can extrapolate from the observed low-dimensional shapes, to build intuition about the high-dimensional distribution. These methods are available in the tourr package (Wickham et al., 2011), including a range of display functions. The package is on CRAN, see <https://CRAN.R-project.org/package=tourr>. The traditional displays are however limited in the case of large data: in scenarios with many observations, overplotting will often hide features, while a large number of variables typically leads to piling of the observations near the center of a projection.

  
In this talk I will introduce new tourr displays that can address these issues. The slice tour (Laa et al., 2020) shows sections of the data, alleviating overplotting issues and potentially revealing concave structures not visible in projections; the sage display (Laa et al., under review, arXiv:2009.10979 ) redistributes the projected data points to reverse piling effects. After introducing the new displays I will briefly describe the implementation in R and show examples that illustrate the advantages of the new approaches.",Data visualisation,6A - Data visualisation
2021,Data science serverless-style with R and OpenFaaS,Regular talks > Track B,UseR! 2021,UseR! 2021,"Solymos, Peter",The Lounge #talk_r_production_1,"R is well suited for data science due to its diverse tooling and its ability to leverage and integrate with other languages and solutions. In production, R is often just a piece of a much larger puzzle providing API endpoints via e.g. plumber, RestRServe, or a similar web framework. Managing many API endpoints can lead to problems due to shifting dependency requirements or more recent additions breaking older code. The common solution is to use Docker containers to provide isolation to these components. However, managing containers at scale is not trivial, and managing serverless infrastructure is often outsourced to public cloud providers. Providers differ in their approaches, leading to independent integrations of R and repeated efforts. The OpenFaaS project was born to mitigate these problems and to avoid vendor lock-in. OpenFaaS is an open-source framework to deploy functions and microservices anywhere (local cluster, public cloud, edge devices) and at any scale (including 0), with an emphasis on Kubernetes. It provides auto-scaling, metrics, API gateway, and is language-agnostic. In this talk, I introduce R templates for OpenFaaS. The templates support different Docker R base images (Debian, Ubuntu, Alpine Linux) and 6 different frameworks, including plumber. I explain the development life cycle with OpenFaaS using an example cloud function for time series forecasting on daily updated epidemiological data. I end with a review of production use cases where R can truly shine in the multilingual serverless landscape.",R in production,6B - R in Production 1
2021,plotVR - walk through your data,Regular talks > Track A,UseR! 2021,UseR! 2021,"Thomann, Philipp",The Lounge #talk_dataviz,"Are you bored by 3D-plots that only give you a simple rotatable 2d-projection? plotVR is an open source package that provides a simple way for data scientists to plot data, pick up a phone, get a real 3d impression - either by VR or by AR - and use the computer's keyboard to walk through the scatter plot:

  
<https://www.github.com/thomann/plotVR>

  
After installing the package and plotting your dataframe, scan the QR code on your phone (iOS or Android) and start walking. Either with recent phones directly in the web browser or using an iOS-app (Android App in prepared).

  
Once you are immersed in your Cardboard how do you navigate through the scatter? plotVR lets you use the computer's keyboard to walk as you would in any first person game.

  
You want to share your impression? Just use the generated USD (iOS) or gltf (Android) files!

  
The technologies beneath this project are: a web server that handles the communication between the DataScience-session and the phone, WebSockets to quickly proxy the keyboard events, QR-codes facilitate the simple pairing of both, and an HTML-Page on the computer to grab the keyboard events. And the translation of these keyboard events into 3D terms is a nice exercise in three.js, OpenGL, and SceneKit for HTML, Android, and iOS resp. For in-browser AR experience the package generates USD and GLTF formats.

  
Ready to see your data as you have never seen before? Join the talk!",Data visualisation,6A - Data visualisation
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_r_production_1,Last questions for all the speakers,Community and Outreach,6B - R in Production 1
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_dataviz,Last questions for all the speakers,Community and Outreach,6A - Data visualisation
2021,startR: A tool for large multi-dimensional data processing,Regular talks > Track A,UseR! 2021,UseR! 2021,"Ho, An-Chi",The Lounge #talk_ecology_environment,"Nowadays, the growing data volume and variety in various scientific domains have made data analysis challenging. Simple operations like extracting data from storage and performing statistical analysis on them have to be rethought. startR is an R package developed at the Earth Science Department in Barcelona Supercomputing Center (BSC-CNS) that allows to retrieve, arrange, and process large multi-dimensional datasets automatically with a concise workflow.

  
startR provides a framework under which the datasets to be processed can be perceived as a single multi-dimensional array. The array is first declared, then a user-defined function can be applied to the relevant dimensions in an apply-like fashion, building up a declarative workflow that can be executed in various computing platforms. During execution, startR implements the MapReduce paradigm, chunking the data and processing them either locally or remotely on high-performance computing systems, leveraging multi-node and multi-core parallelism where possible. Besides the data, metadata are also well-preserved and expanded with the operation information, ensuring the reproducibility of the analysis.

  
Several functionalities in startR, like spatial interpolation and time manipulation, are tailored for atmospheric sciences research such as climate, weather, and air quality. It is compatible with other R tools developed in BSC-CNS, forming a strong toolset for climate research. However, it is potentially competent in other research fields. Even though netCDF is the only data format supported in the current release, adaptors for other file formats can be plugged in, enabling the tool to be exploited in different scientific domains where large multi-dimensional data is involved.","Ecology, Environmental sciences",7A - Ecology and Environmental Sciences
2021,Probability Distribution Forecasts: Learning with Random Forests and Graphical Assessment,Regular talks > Track B,UseR! 2021,UseR! 2021,"Lang, Moritz N.",The Lounge #talk_stats,"Forecasts in terms of entire probability distributions (often called """"probabilistic forecasts"""" for short) - as opposed to predictions of only the mean of these distributions - are of prime importance in many different disciplines from natural sciences to social sciences and beyond. Hence, distributional regression models have been receiving increasing interest over the last decade. Here, we make contributions to two common challenges in distributional regression modeling:

1. Obtaining sufficiently flexible regression models that can capture complex patterns in a data-driven way.
2. Assessing the goodness-of-fit of distributional models both in-sample and out-of-sample using visualizations that bring out potential deficits of these models.

Regarding challenge 1, we present the R package """"disttree"""" (Schlosser et al. 2021), that implements distributional trees and forests (Schlosser et al. 2019). These blend the recursive partitioning strategy of classical regression trees and random forests with distributional modeling. The resulting tree-based models can capture nonlinear effects and interactions and automatically select the relevant covariates that determine differences in the underlying distributional parameters.

For graphically evaluating the goodness-of-fit of the resulting probabilistic forecasts (challenge 2), the R package """"topmodels"""" (Zeileis et al. 2021) is introduced, providing extensible probabilistic forecasting infrastructure and corresponding diagnostic graphics such as Q-Q plots of randomized residuals, PIT (probability integral transform) histograms, reliability diagrams, and rootograms. In addition to distributional trees and forests other models can be plugged into these displays, which can be rendered both in base R graphics and """"ggplot2"""" (Wickham 2016).",Statistical models,7B: Statistical modeling in R
2021,A semi-automatic grader for R scripts,Regular talks > Track C,UseR! 2021,UseR! 2021,"Gopal, Vik",The Lounge #talk_teaching_automation_reproducibility,"My department teaches a class in R. The aims of this class are to teach visualisation and good programming practices in R. Every week, we would attempt to go over as many script submissions as we could, as closely as we could. We would then summarise the feedback verbally to the students.

  
Due to the increasing class size, we were unable to rigorously go through every single student script every week due to time constraints. As such, we could not identify the common misconceptions that students had. We could not intervene and correct the most critical ones early one in the class. Finally, we were unable to visualise all the visualisations that students created.

  
Hence we developed an R package to automatically run all student scripts and extract metrics such as run-time and certain code features. The package would also collate all the graphs so that we can see them at one go. We also set up a server for students to test their code before submission, ensuring that we can run their code smoothly.

  
We can now ensure that every students’ code is run and analysed consistently and reliably. Instead of scrutinising the code, we look through a summary table of features generated for each script. If something looks strange here, we go back to the script. By uploading this table, with comments, to our LMS, we can provide custom feedback for each student. Finally, having such a summary table of features indicates the areas that students need more practice in - it allows us to tailor future homework problems.","Reproducibility, Teaching R/R in Teaching","7C - Teaching, Automation and Reproducibility"
2021,grwat: a new R package for automated separation and analysis of river hydrograph,Regular talks > Track A,UseR! 2021,UseR! 2021,"Samsonov, Timofey",The Lounge #talk_ecology_environment,"grwat is a new R package aimed at analysis of river hydrograph — a time series of river discharge values. The overall shape of hydrograph is specific for each river and is heavily influenced by climatic conditions within a river basin. Since the climate is changing, the shape of a typical hydrograph for each river is also transformed. The main goal of grwat package is to provide automated tools to extract the genetic components of river discharge (e.g. how much disharge is due to thaws, floods etc.) as well as graphical and statistical tools to reveal interannual and long-term changes of these components. The core procedure which allows extraction of genetic components is separation. The implementation of separation in grwat is two-stage. First, it follows the generaly acclaimed approach to separate the discharge into quick flow and baseflow. Second, it involves the temperature and precipitation time series to separate the quick flow into seasonal (snowmelting), thaw and flood-induced discharge using the originally developed algorithm. The separation is programmed in pure STL C++17 and then interfaced into grwat via Rcpp. Separated hydrograph is represented as a data frame where for each observation the input total discharge is distributed between several columns, each representing a genetic component. Such data frame can be further analyzed with grwat resulting in more than 30 interannual and long-term statistically tested variables characterizing the aggregated values, dates and durations of specific events and periods. Examples are seasonal flood runoff, annual groundwater discharge, number of thaw days, and the date of seasonal flood beginning. Finally, grwat contains convenient functions to quickly visualize one or more variables using ggplot2 graphics, and to generate high-quality R Markdown-based HTML reports which combine graphics and results of statistical tests for all computed variables. Development is funded by Russian Science Foundation (Project 19-77-10032)","Ecology, Environmental sciences",7A - Ecology and Environmental Sciences
2021,"spaMM: an R package to fit generalized, linear, and mixed models allowing for complex covariance structures",Regular talks > Track B,UseR! 2021,UseR! 2021,"Rousset, François",The Lounge #talk_stats,"Introduced to make the fit of spatial Mixed Models accessible, the R package spaMM has grown a lot since its first CRAN release eight years ago. The package now offers the possibility to fit a variety of regression models, from simple linear models (LM) to generalised linear mixed-effects models (GLMM), including multivariate-response models, and double hierarchical GLMMs (DHGLM) in which both the mean of a response and the residual variance can be modelled as a function of fixed and random effects. The package provides a diversity of response families beyond the standard ones, such as the (truncated or not) negative binomial, and the Conway-Maxwell-Poisson, as well as non-gaussian random effect families such as the inverse gaussian. Random effects can further be modelled using several autocorrelation functions for the consideration of spatial, temporal and other forms of dependence between observations (e.g. genetic pedigrees). spaMM handles this diversity of models through a simple formula-based interface akin to glm() or lme4::glmer(). Advanced users will nonetheless appreciate the possibility to fine tune many aspects of the fit (e.g. select among several likelihood approximations; set parameters to fixed values). The package also provides tailored methods for many generics, so that for instance anova() can be called to perform likelihood ratio tests by parametric bootstrap and that AIC() computes both the marginal and conditional AIC. The package is finally competitive in terms of computational speed, for both non-spatial, geostatistical, and autoregressive models",Statistical models,7B: Statistical modeling in R
2021,Automating bespoke online teaching with R,Regular talks > Track C,UseR! 2021,UseR! 2021,"Davies, Rhian",The Lounge #talk_teaching_automation_reproducibility,"At Jumping Rivers we deliver over 100 R, Python and Stan training courses each year, engaging with thousands of new learners. The necessity to move to fully online training in March last year meant we quickly had to completely rethink how to deliver R training interactively online. We internally trialled running our usual in-person training just on Zoom - and it really doesn’t work trust us!

We already used R &amp; R Markdown to create all training materials including slides and notes.

However, our new workflow uses R in every step of the way, from creating a bespoke learning environment, to collating feedback and generating certificates.

Upcoming training sessions are stored in Asana. Using a single call from R, we extract the relevant Asana task details and:

- Provide the client with a single URL that contains all necessary information for the course
- Deploy a bespoke virtual training environment with {analogsea}
- Automate password generation with {shiny}
- Track and upload attendance sheets
- Create bespoke Google Documents for code quizzes
- Generate fill-in-the-blank tutor R scripts
- Provide automatic feedback reports for clients with {rmarkdown}, {shiny} &amp; {rtypeform}
- Deliver personalised certificates in {shiny}
- Tag the training materials and VM to enable a completely reproducible set-up.

This improves the learning experience as the “small” things are automated and allows the trainer to concentrate on actual training.","Reproducibility, Teaching R/R in Teaching","7C - Teaching, Automation and Reproducibility"
2021,Using R6 object-oriented programming to build agent-based models,Regular talks > Track A,UseR! 2021,UseR! 2021,"Bailey, Liam Daniel",The Lounge #talk_ecology_environment,"Agent or individual-based modelling is an invaluable tool in the biological sciences, used to understand complex topics such as conservation management, invasive species, and animal population dynamics. However, while R is one of the most common programming languages used in the biological sciences it is often considered 'unsuitable' for agent-based modelling tasks, with other tools such as NetLogo, Java, and C++ utilized instead. Here, we introduce how the package R6 can be used to build agent-based models and simulate complex population and evolutionary dynamics in R. R6 offers the possibility to easily define classes with encapsulated methods. It has become the package of choice behind many well-known R packages that use encapsulated object-oriented programming (e.g. shiny, dplyr, testthat). Yet, while simulations have been built in R using other class systems such as S3 and S4, the potential of R6 to perform such tasks remains untapped. We provide a real-world example from our research on the large African carnivore, the spotted hyena. Object-oriented programming using R6 was easy to learn and implement, and working in R allowed us to quickly build, document, and unit test our code by taking advantage of existing tools in R/RStudio with which we were already familiar (e.g. RStudio projects, roxygen2, testthat). Implementing agent-based modelling in R will allow ecologists to easily make use of this powerful tool in their research. Researchers will not be required to learn any new programming languages but can instead implement agent-based models in the same language they already use for data wrangling, statistical analysis, and data visualisation.","Ecology, Environmental sciences",7A - Ecology and Environmental Sciences
2021,The one-step estimation procedure in R,Regular talks > Track B,UseR! 2021,UseR! 2021,"Brouste, Alexandre",The Lounge #talk_stats,"In finite-dimensional parameter estimation, the Le Cam one-step procedure is based on an initial guess estimator and a Fisher scoring step on the loglikelihood function. For an initial $sqrt(n)$–consistent guess estimator, the one-step estimation procedure is asymptotically efficient. As soon as the guess estimator is in a closed form, it can also be computed faster than the maximum likelihood estimator. More recently, it has been shown that this procedure can be extended to an initial guess estimator with a slower speed of convergence. Based on this result, we propose in the OneStep package (available on CRAN) a procedure to compute the one-step estimator in any situation faster than the MLE for large datasets. Monte-Carlo simulations are carried out for several examples of statistical experiments generated by i.i.d. observation samples (discrete and continuous probability distributions). Thereby, we exhibit the performance of Le Cam’s one-step estimation procedure in terms of efficiency and computational cost on observation samples of finite size. A real application and the future package developments will also be discussed.",Statistical models,7B: Statistical modeling in R
2021,Extend the functionality of your R Markdown documents,Regular talks > Track C,UseR! 2021,UseR! 2021,"Dervieux, Christophe",The Lounge #talk_teaching_automation_reproducibility,"R Markdown is a powerful tool that has quickly grown since its creation. If it can be rather simple to quickly create and maintain a simple reproducible report, it can be more challenging to do advanced customization and dynamic content creation due to the different tools involved (rmarkdown, knitr, Pandoc, LaTeX, ...) and a lot of possible tweaks. And this is increaded all the more if you consider the already widespread and still growing ecosytem surrounding R Markdown.

  
Helping users to better find and know how to do specific tasks with R Markdown was the main driver for the book """"R Markdown Cookbook"""" (CRC Press). This talks will be based on the content of this book and will present a selection of advanced recipes to go further with a R Markdown document. These examples combines little-known features of some R packages (rmarkdown, knitr) and other tools (Pandoc) to provide flexibility and to extend greatly the functionnality for producing communication product, programmatically and with reproducibility.

  
This talks will also include last features at the time of the talk included in R Markdown family of packages (rmarkdown, knitr, bookdown, blogdown, ...)","Reproducibility, Teaching R/R in Teaching","7C - Teaching, Automation and Reproducibility"
2021,Climate Forecast Analysis Tools Framework: from the storage to the HPC to get reproducible climate research results and services,Regular talks > Track A,UseR! 2021,UseR! 2021,"Pérez-Zanón, Núria",The Lounge #talk_ecology_environment,"Climate forecast researchers need to assess the quality of their forecasts by comparing them against reference observation datasets using state-of-the-art verification metrics. This procedure requires reading in the seasonal forecasts and reference data and restructuring them for later comparison (e.g.: regridding, resampling or reordering). Only then, statistical methods can be applied to assess forecast skill and, finally, tailored visualization tools are employed to explore the results.

  
At the Earth Sciences department of the Barcelona Supercomputing Center, the expertise in seasonal forecast research has traditionally been compiled in the s2dverification R package since its first release in 2009. The package provides tools implementing all the steps required for the procedure described above, allowing researchers to share their methods while reducing development and maintenance cost. However, as the department broadened its activity to include research on sub-seasonal forecast, decadal prediction and climate projections, as well as development of climate services for various stakeholders, new state-of-the-art tools to manipulate climate data became necessary.

  
As a result, the department is currently maintaining eight R packages. These packages can be used separately or in their common framework, and include methods for calibration, downscaling and combination in the CSTools package, climate indicators in ClimProjDiags, and CSIndicators -among other climatological methods- in s2dv (s2dverification’s successor). The framework has been designed to be flexible and efficient. The Big Data issue inherent to climate data analysis is addressed by employing the startR and multiApply packages to seamlessly enable chunked multi-core processing, optionally leveraging multi-node parallelism in HPC platforms.","Ecology, Environmental sciences",7A - Ecology and Environmental Sciences
2021,The R Package stagedtrees for Structural Learning of Stratified Staged Trees,Regular talks > Track B,UseR! 2021,UseR! 2021,"Varando, Gherardo",The Lounge #talk_stats,"stagedtrees is an R package which includes several algorithms for learning the structure of staged trees and chain event graphs from categorical data. In the past twenty years there has been an explosion of the use of graphical models to represent the relationship among a vector of random variables and perform inference taking advantage of the underlying graphical representations.

Bayesian networks are nowadays one of the most used graphical models, with applications to a wide array of domains and implementation in various software. However, they can only represent symmetric conditional independence statements which in practical applications may not be fully justified. Most often, the greater the number of levels of categorical variables involved, the more difficult it is for conditional independence to hold for all the variables’ levels. Therefore, models accommodating also asymmetric relations as context-specific, partial and local independences have been developed. Staged trees are one such class. Staged tree modeling has proved its worth in many fields, as for instance cohort studies, causal analysis, case-control studies, Bayesian games and medical diagnosis.

stagedtrees permits to estimate any type of non-symmetric conditional independences from data via score-based and clustering-based algorithms. Various functionalities to provide inferential, visualization, descriptive and summary statistics tools for such models and about their graph structure are implemented. These functions help users in handling categorical experimental data and analyzing the learned models to untangle complex dependence structures.",Statistical models,7B: Statistical modeling in R
2021,Computational aspects of psychometrics taught with R and Shiny,Regular talks > Track C,UseR! 2021,UseR! 2021,"Martinkova, Patricia",The Lounge #talk_teaching_automation_reproducibility,"Psychometrics deals with the advancement of quantitative measurement practices in psychology, education, health, and many other fields. It covers a number of statistical methods that are useful for the behavioral and social sciences. Among other topics, it includes the estimation of reliability to deal with the omnipresence of measurement error, as well as a more detailed description of item functioning encompassed in item response theory models.

  
In this talk, I will discuss some computational aspects of psychometrics, and how understanding these aspects may be supported by real and simulated datasets, interactive examples, and hands on methods. I will first focus on reliability estimation and the issue of restricted range, showing that zero may not always be zero. I will then focus on a deeper understanding of the context behind more complex models and their much simpler counterparts. The last example discusses group-specific models and the importance of item-level analysis for situations where differences in overall gains are not apparent but the differences in item gains may be.

  
I will finally discuss experiences from teaching computational aspects of psychometrics to a diverse group of students from various fields, including statistics, computer science, psychology, education, medicine, and participants from industry. I will discuss the challenges and joys of creating a truly interdisciplinary course.","Reproducibility, Teaching R/R in Teaching","7C - Teaching, Automation and Reproducibility"
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_ecology_environment,Last questions for all the speakers,Community and Outreach,7A - Ecology and Environmental Sciences
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_stats,Last questions for all the speakers,Community and Outreach,7B: Statistical modeling in R
2021,Questions,Regular talks > Track C,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_teaching_automation_reproducibility,Last questions for all the speakers,Community and Outreach,"7C - Teaching, Automation and Reproducibility"
2021,MolEvolvR: Web-app and R-package for characterizing proteins using molecular evolution and phylogeny,Regular talks > Track A,UseR! 2021,UseR! 2021,"Ravi, Janani",The Lounge #talk_stats_bioinformatics,"Molecular evolution and phylogeny can provide key insights into pathogenic protein families. Studying how these proteins evolve across bacterial lineages can help identify lineage-specific and pathogen-specific signatures and variants, and consequently, their functions. We have developed a streamlined computational approach for characterizing the molecular evolution and phylogeny of target proteins, widely applicable across proteins and species of interest. Our approach starts with query protein(s) of interest, identifying their homologs, and characterizing each protein by its domain architecture and phyletic spread. We have developed the MolEvolvR webapp, written entirely in R and Shiny, to enable biologists to run our entire workflow on their data by simply uploading a list of their proteins of interest. The webapp accepts inputs in multiple formats: protein/domain sequences, multi-protein operons/homologous proteins, or motif/domain scans. Depending on the input, MolEvolvR returns the complete set of homologs/phylogenetic tree, domain architectures, common partner domains. Users can obtain graphical summaries that include multiple sequence alignments and phylogenetic trees, domain architectures, domain proximity networks, phyletic spreads, co-occurrence patterns, and relative occurrences across lineages. Thus, the MolEvolvR webapp provides a powerful, easy-to-use interface for a wide range of protein characterization analyses, starting from homology searches and phylogeny to domain architectures. In addition to this analysis, researchers can use the app for data summarization and dynamic visualization. The webapp can be accessed here: <http://jravilab.org/molevolvr>. Soon, it will be available as an R-package for use by computational biologists.","Bioinformatics / Biomedical or health informatics, Biostatistics and Epidemiology, Data visualisation, Web Applications (Shiny/Dash)",8A - Statistics and Bioinformatics
2021,Shiny PoC to Production Application in 8 steps,Regular talks > Track B,UseR! 2021,UseR! 2021,"Dubel, Marcin",The Lounge #talk_r_production_2,"""""A great advantage of Shiny applications is that a proof of concept can be created quickly and easily. It is a great way for subject matter experts to present their ideas to stakeholders before moving on to production. However, taking the next step to a production application requires help from experienced software developers. The actions should be focused on two areas: to make the application a great experience for users and to make it maintainable for future work. Focusing on these will assure that the app will be scalable, performant, bug-free, extendable, and enjoyable. Close collaboration between engineers and experts paves a wave to many successful projects in data science and is Appsilon’s confirmed path to production-ready solutions.

  
The very first step should always be to build a comfortable and (importantly) reproducible workflow, thus setting up the development environment and organizing the folder structure \[renv + docker\]. Once this is done engineers should proceed to limiting the codebase by cleaning the code – i.e., removing redundant comments, extracting the constants and inline styles \[ymls + styler\]. Now the fun begins: extract the business logic into separate functions, modules and classes \[packages/R6 + plumber\]. Restrict reactivity to minimum. Check the logic \[data.validator + drake\]. Add tests \[testthat + cypress/shinytest\]. Organize your /www and move actions to the browser \[shiny + css/js\]. Finally, style the app \[sass/bslib + shiny.fluent\]. And, voila! A world-class Shiny app.""""",R in production,8B - R in Production 2
2021,"fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation",Regular talks > Track C,UseR! 2021,UseR! 2021,"Wiśniewski, Jakub",The Lounge #talk_stats_data_analysis_1,"Machine learning decision systems are getting increasingly omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are very eager to learn social biases present in historical data that can lead to increasing discrimination. If we want to create models responsibly then we need tools for in-depth validation of models also from the perspective of potential discrimination.

  
This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in classification models in an easy and flexible fashion. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented set of functions and fairness metrics enables model fairness validation from different perspectives. The package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.

  
The package is designed not only to examine a single model but also to facilitate comparisons between multiple models.","Data mining / Machine learning / Deep Learning and AI, Multivariate analysis",8C - Statistical modeling &  Data Analysis 1
2021,Using R to Empower a Precision Dosing Web Application,Regular talks > Track A,UseR! 2021,UseR! 2021,"Daroczi, Gergely",The Lounge #talk_stats_bioinformatics,"R has a long history in PK/PD modeling, and it has been heavily used both in research and clinical practice as well, but making these R packages available outside of the R community has its (technical, compliance and UX) challenges that even hosted Shiny apps cannot easily solve yet. We are working on and presenting a scalable platform and web application building on the top of R (Docker, containerized Plumber API), hosted in a HIPAA-compliant infrastructure (AWS and GCP services), and made available to end-users via a user-friendly and configurable web interface (Angular and Nebular). This talk will focus on the overall cloud infrastructure, how we integrate R and other services, and details on the troubles with scalability, error handling, user experience etc in a HIPAA compliant, but startup environment.","Bioinformatics / Biomedical or health informatics, Biostatistics and Epidemiology, Data visualisation, Web Applications (Shiny/Dash)",8A - Statistics and Bioinformatics
2021,Reliably Reproducible Project Packages,Regular talks > Track B,UseR! 2021,UseR! 2021,"Gold, Alex Kahn",The Lounge #talk_r_production_2,"We all dread sharing a data science project with a collaborator or returning to a project only to find that it doesn't run because of mismatched package versions. Maintaining and sharing R projects is historically a fragile endeavor, relying mainly on crossed fingers.

  
There are now simple workflows to create and share an isolated R package environment for any project that makes luck irrelevant to the process.

  
In this talk, you'll learn to use the {renv} package to easily and quickly create isolated project environments, capture the packages in those environments, and share them with collaborators. Additionally, you'll learn how to take advantage of dated repository URLs from public RStudio Package Manager to make sure that you can add more packages and continue work on your project, no matter how far down the road that is.",R in production,8B - R in Production 2
2021,DoubleML - Double Machine Learning in R,Regular talks > Track C,UseR! 2021,UseR! 2021,"Bach, Philipp; Kurz, Malte S.",The Lounge #talk_stats_data_analysis_1,"The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). DoubleML makes it possible to estimate causal parameters based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML allows users to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This talk serves as an introduction to the double machine learning framework and the R package DoubleML. We demonstrate how users of DoubleML can perform valid inference based on machine learning methods in reproducible code examples with simulated and real data sets.

  
References:

  
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68, URL: <https://academic.oup.com/ectj/article/21/1/C1/5056401>.

  
Lang, M., Binder, M., Richter, J., Schratz, P., Pfisterer, F., Coors, S., Au, Q., Casalicchio, G., Kotthoff, L. and Bischl, B. (2019), mlr3: A modern object-oriented machine learing framework in R. Journal of Open Source Software, doi:10.21105/joss.01903, URL: <https://mlr3.mlr-org.com/>.","Data mining / Machine learning / Deep Learning and AI, Multivariate analysis",8C - Statistical modeling &  Data Analysis 1
2021,Visualization of highly-multiplexed imaging data with cytomapper,Regular talks > Track A,UseR! 2021,UseR! 2021,"Eling, Nils",The Lounge #talk_stats_bioinformatics,"Highly multiplexed imaging (HMI) produces images that contain up to 40 channels. In the field of cell biology, HMI is used to capture differences between individual cells, which are defined as distinct objects on the images. To derive those objects from multi-channel images, different segmentation approaches can be used. Several challenges in terms of data visualisation arise from this type of high dimensional data: 1. more than 3 channels need to be visualised at once, 2. the features of segmented objects need to be visualised together with pixel-level information and 3. tens to hundreds of images need to be visualised in parallel. Here, we have developed cytomapper, an R/Bioconductor package to address these challenges. The main functions of the package allow 1. the visualisation of pixel-level information across multiple channels, 2. the display of object-level information on segmentation masks and 3. the interactive visualization of images based on an integrated shiny application. Finally, we also developed an on-disk representation framework to expand the usability of the package to several hundreds of images.","Bioinformatics / Biomedical or health informatics, Biostatistics and Epidemiology, Data visualisation, Web Applications (Shiny/Dash)",8A - Statistics and Bioinformatics
2021,"Binary R Packages for Linux: Past, Present and Future",Regular talks > Track B,UseR! 2021,UseR! 2021,"Ucar, Iñaki",The Lounge #talk_r_production_2,"Pre-compiled binary packages provide a very convenient way of efficiently distributing software that has been adopted by most Linux package management systems. However, the heterogeneity of the Linux ecosystem, combined with the growing number of R extensions available, poses a scalability problem. As a result, efforts to bring binary R packages to Linux have been scattered, and lack a proper mechanism to fully integrate them with R’s package manager. This work reviews past and present of binary distribution for Linux, and presents a path forward by showcasing the ‘cran2copr’ project, an RPM-based proof-of-concept implementation of an automated scalable binary distribution system with the capability of building, maintaining and distributing thousands of packages, while providing a portable and extensible bridge to the system package manager. This not only benefits desktop/server users of Linux systems, but also Windows and macOS users that rely on CI/CD systems to test packages and/or deploy code.",R in production,8B - R in Production 2
2021,copent: Estimating Copula Entropy and Transfer Entropy in R,Regular talks > Track C,UseR! 2021,UseR! 2021,"MA, Jian",The Lounge #talk_stats_data_analysis_1,"Statistical independence and conditional independence are two fundemental concepts in statistics and machine learning. Copula Entropy is a mathematical concept for multivariate statistical independence measuring and testing, and also proved to be closely related to conditional independence (or transfer entropy). It has been applied to solve several fundamental statistical or machine learning problems, including association discovery, structure learning, variable selection, and causal discovery. The method for estimating copula entropy with rank statistic and the kNN method was implemented in the 'copent' package in R. This talk first introduces the theory and estimation of Copula Entropy, and then the implementation details of the package. Three examples will also be presented to demonstrate the usage of the package: one with simulated data and the other two with real-world data for variable selection and causal discovery. The copent package is available on the CRAN and also on GitHub at <https://github.com/majianthu/copent/>.","Data mining / Machine learning / Deep Learning and AI, Multivariate analysis",8C - Statistical modeling &  Data Analysis 1
2021,Sponsored Talk by Roche,Regular talks > Track A,UseR! 2021,UseR! 2021,"Copping, Ryan",The Lounge #talk_stats_bioinformatics,TBA,"Bioinformatics / Biomedical or health informatics, Biostatistics and Epidemiology, Data visualisation, Web Applications (Shiny/Dash)",8A - Statistics and Bioinformatics
2021,Questions,Regular talks > Track B,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_r_production_2,Last questions for all the speakers,Community and Outreach,8B - R in Production 2
2021,Questions,Regular talks > Track C,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_stats_data_analysis_1,Last questions for all the speakers,Community and Outreach,8C - Statistical modeling &  Data Analysis 1
2021,Questions,Regular talks > Track A,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_stats_bioinformatics,Last questions for all the speakers,Community and Outreach,8A - Statistics and Bioinformatics
2021,Incubator: Expanding the R community in the Middle East and North Africa (MENA),Panels and incubators,UseR! 2021,UseR! 2021,"Almarzouq, Batool",The Lounge #incubator_mena,"Data Science (DS) has become an in-demand, highly paid career in MENA, especially in UAE, KSA and Oil-rich Gulf countries but despite this interest in DS in the Middle East and North Africa (MENA), there is very few R meet-up/R user group dedicated to learning, teaching, and sharing information about the R Programming language in MENA. Therefore, three chapters for R-Ladies and two R User groups were established in Saudi Arabia, Tunis, Algeria and Egypt last year (2020) to bring out more R users and empower data scientist from underrepresented communities in MENA. However, the growth of the R community in MENA is limited by the language barrier, which makes the communities that exist around R (e.g. MiR, ..) not well-known in MENA. This incubator aims to establish a working group dedicated to expanding the R community in MENA, and it will engage researchers and data scientist in MENA through in-person collaboration on open source projects. It'll also publish a guide in Arabic to be delivered directly to various Universities and research centres in MENA.

  
By the end of the incubator, we'll have established a working group and identified the barriers and how to overcome them to expand the R community in MENA. This working group will initiate a set of open-source R projects to facilitate collaboration and integration between data scientists in MENA and communities around R Worldwide.",Community and Outreach,Incubator: Expanding the R community in the Middle East and North Africa (MENA)
2021,Incubator: Stop reinventing the wheel: R package(s) for conference and abstract management,Panels and incubators,UseR! 2021,UseR! 2021,"Bannert, Matthias; Ravi, Janani",The Lounge #incubator_conference_abstract,"The ability to host an entire conference online went from nice-to-have to absolutely essential for many communities within just a bit more than a year. As a consequence, online conference tools were exposed to a wider audience, faced tougher scrutiny, and hauled in more feedback than ever before.

  
So far no clear front runner among conference tools has emerged from the process. Strikingly, all current conference tools used by the open-source community have major deficiencies such as accessibility issues, are bulky to navigate for admins, and/or come with a hefty price tag. The idea of this incubator is to discuss whether and how processes such as registration, abstract evaluation, or submission can be mapped into an R package and the open-source ecosystem. We aim to challenge established processes and walk through possible process changes, e.g., reviews. We also see such a system as an opportunity towards equitable practices in picking topics, forming committees, choosing reviewers, providing feedback to submitters, and selecting contributions. We intend to work on a solution that is easy to reproduce and facilitates knowledge transfer for annual conferences with changing organizers.",R in production,Incubator: Stop reinventing the wheel: R package(s) for conference and abstract management
2021,Production Metrics for R with the 'openmetrics' Package,Regular talks > Track A,UseR! 2021,UseR! 2021,"Jacobs, Aaron",The Lounge #talk_r_production_3,"Production applications are often expected to emit """"metrics"""" so that they can be monitored in real time for problems. However, traditional monitoring vendors expected developers to use their proprietary client libraries, none of which included support for R. This left R users without the option of supporting their organisation's existing vendor, and monitoring support for R applications (such as Shiny apps or Plumber APIs) has remained poor.

  
Over the last few years, the open-source Prometheus project has become the de facto metrics solution in the Kubernetes community, and its text-based format (now called 'OpenMetrics') has been formalised as a draft IETF standard. This effort is the closest thing in the monitoring community to a widely-accepted standard in its history.

  
This talk will introduce the {openmetrics} package, a client library for the OpenMetrics standard. It allows R applications to expose metrics to any Prometheus instance, or one of the growing number of open-source and commercial tools that can ingest the OpenMetrics format.

  
In addition to user-defined, application-specific metrics, the package also bundles general-purpose metrics and can automatically inject them into Shiny apps or Plumber APIs. This makes it easy for R users to add production-ready metrics to their applications in only a few lines of code.

  
As production use of R grows, expectations about production features -- such as metrics and corresponding alerts -- will grow as well.","R in production, R in the wild (unusual applications of R)",9A - R in production 3
2021,"Using R in Latin America: the great, the good, the bad, and the ugly",Regular talks > Track B,UseR! 2021,UseR! 2021,"García Alonso, Virginia A.; Corrales, Paola; Huaylla, Claudia A.; Gómez Vargas, Andrea; Chávez, Joselyn; Fierro Arcos, Denisse",The Lounge #talk_community_outreach_2,"R is used globally for diverse purposes. However, how widely is it used in peripheral countries? What barriers and challenges are faced there? We are a group of R useRs from Latin America, a region facing several barriers: language, infrastructure, distance, different access to resources, information and training.

  
This inequality often represents not only an additional barrier for people to learn and use R, but also an obstacle to participate in international events which reduces our representation on the global sphere. Before improving the inclusion of Latin Americans in the global R community, we need to comprehensively understand useRs’ experiences. With this aim in mind, we created an online survey designed to assess which challenges Latin American R users face that has been shared within a strong existing network of Latin American useRs from Tijuana to Ushuaia.

  
Over 900 useRs completed the survey and preliminary results already highlight that several useRs from this region often face challenges associated with not being native English speakers or with lacking access to certain resources. However, it also revealed that many belong to different R communities such as R-Ladies and LatinR among others, serving as useful platforms to connect with others who can help to cope with the shared difficulties they face.

  
In this talk we will present the analysis and results from this survey and invite others to do the same as this open and reusable initiative seeks to inspire other underserved regions around the world to measure their strengths and challenges to help increase inclusion and diversity of the international R community.",Community and Outreach,9B Community and Outreach 2
2021,bssm: Bayesian Inference of Non-linear and Non-Gaussian State Space Models in R,Regular talks > Track C,UseR! 2021,UseR! 2021,"Helske, Jouni",The Lounge #talk_stats_data_analysis_2,"State space models are a flexible class of latent variable models commonly used in analysing time series data. The R package bssm is designed for Bayesian inference of general state space models with non-Gaussian and/or non-linear observational and state equations. The package provides easy-to-use and efficient functions for fully Bayesian inference with common time series models such as basic structural time series model with exogenous covariates, simple stochastic volatility models, and discretized diffusion models, making it straightforward and efficient to make predictions and other inference in a Bayesian setting. Unlike the existing packages, bssm allows for easy-to-use approximate inference based on Gaussian approximations such as the Laplace approximation and the extended Kalman filter. The inference is based on fully automatic, adaptive Markov chain Monte Carlo (MCMC) on the hyperparameters, with optional parallelizable importance sampling post-correction to eliminate any approximation bias. The bssm package implements also a direct pseudo-marginal MCMC and a delayed acceptance pseudo-marginal MCMC using intermediate approximations. The package supports directly models with linear-Gaussian state dynamics with non-Gaussian observation models and has an Rcpp interface for specifying custom non-linear and diffusion models.","Bayesian models, Social sciences, Statistical models, Time series",9C - Statistical modeling and data analysis 2
2021,Automating business processes with R,Regular talks > Track A,UseR! 2021,UseR! 2021,"van Dunné, Frans",The Lounge #talk_r_production_3,"At ixpantia we help organizations to become their most innovative and data-driven selves through personalized coaching and knowledge transfer, continuous and transparent code handover and the implementation of an efficient and cooperative data culture. To bring the results of data analysis to business processes and decision making, we usually need to automate their execution. Often this means that we need a daily process to write, for example, predicted values to a database.

  
The possibilities to add value to organizations through the use of tools that are available to us in R are legion. This value lies not only in advanced analytics, but also in the power of dynamic (automated) reports and pre-calculated values that combine multiple formal and informal data sources. The value of R for automating these tasks is often so high because the domain experts themselves are writing it, and can iterate at high speeds to answer to changes in their business context.

  
In this talk we will share some of the experiences we have had automating tasks with R. We will also present the pragmatic approach to run and monitor scheduled tasks that we have developed. This approach is based on Rmarkdown, a taskscheduler (such as cron) and a Shiny to monitor task execution and completion.","R in production, R in the wild (unusual applications of R)",9A - R in production 3
2021,r-community.org: a central community infrastructure for R,Regular talks > Track B,UseR! 2021,UseR! 2021,"UBAH, CHIBUOKEM BEN",The Lounge #talk_community_outreach_2,"Large amounts of user data are generated from several social and technology spaces within the R Community daily, providing information about important trends and insights that could be helpful in planning for the future of a sustainable open-source community.

  
This presentation is about r-community.org, a website for tracking R related data from popular online spaces like MEETUP, CRAN, GITHUB, TWITTER, STACKOVERFLOW &amp; R-BLOGGERS with the purpose of understanding time-related trends (where applicable). The visualizations from this tool are planned to provide an easy way for newbies/oldies to the R language to discover communities and events that are available to them, while allowing an exploration of past and future R Conferences/meetups for all. The infrastructure relies on open-source R packages and continuous integration technology for regular update of data.

  
With the results of this tool, users, organizations and sub-communities are able to find extra insights for driving decision-making regarding developments around the R language and its global community.",Community and Outreach,9B Community and Outreach 2
2021,bnmonitor: Checking the Robustness and Sensitivity of Bayesian Networks,Regular talks > Track C,UseR! 2021,UseR! 2021,"Wilkerson, Rachel",The Lounge #talk_stats_data_analysis_2,"Bayesian networks (BNs) are the most common approach to investigate the relationship between random variables. There are now a variety of R packages with the capability of learning such models from data and performing inference. The new bnmonitor R package is the only package which enables users to perform robustness and sensitivity analysis for BNs, both in the discrete and in the continuous case.

  
Various prequential monitors are implemented to check how well a BN describes a dataset used to learn the model. By checking the elements of the structure, we can adjust the model, presumably the best in the equivalence class, to ensure a good fit. Checking the forecasts that flow from the model allows users to check elements of the model structure in an online setting.

  
Furthermore the impact of the learned probabilities is investigated using sensitivity functions which describe the functional relationship between an output of interest and the model's parameters.

  
The output of these monitors are concisely reported via a tailored plot method taking advantage of ggplot2. We illustrate our methods with an example that explores the relationships between body measurements to predict the percentage of body fat. Our example highlights the importance of checking a BN with the appropriate diagnostics.","Bayesian models, Social sciences, Statistical models, Time series",9C - Statistical modeling and data analysis 2
2021,RStudio Managed Workbench -,Regular talks > Track A,UseR! 2021,UseR! 2021,"Schratz, Patrick",The Lounge #talk_r_production_3,"As a certified RStudio partner, we offer to deploy a full-fledged RStudio Workbench installation on your infrastructure (on-premise) or in the cloud.

  
In this talk we showcase the benefits of running a centralised RStudio Workbench installation and explain the added value of our containerised service compared to the default binary installation of RStudio Workbench.","R in production, R in the wild (unusual applications of R)",9A - R in production 3
2021,CovidR and rmdgallery: A streamlined process for collecting community contributions in a gallery website,Regular talks > Track B,UseR! 2021,UseR! 2021,"Porreca, Riccardo",The Lounge #talk_community_outreach_2,"The active open source community is certainly one of R’s greatest assets. During the organization of the virtual e-Rum2020, in the midst of the pandemic outbreak, the Organizing Committee thought of engaging the R community by gathering contributions around the topic of COVID-19, as part of the pre-conference event CovidR (<https://2020.erum.io/covidr-contest>). This came with the need for a smooth and integrated way of collecting community work, which was elegantly addressed in the form of a contributions gallery website (<https://milano-r.github.io/erum2020-covidr-contest>), populated with submissions coming as Pull Requests / Issues in a GitHub repository.

  
Behind the scenes, this was supported by the development of a novel package rmdgallery (<https://riccardoporreca.github.io/rmdgallery>), providing an R markdown site generator for a gallery of (embedded) content created in a dynamic way based on metadata in YAML or JSON format. This simple yet flexible tool, paired with GitHub Actions and the community features of GitHub, was key to the CovidR success, in a number of ways. These include: seamless submission process, automated inclusion in the gallery of contributed abstracts and content, collecting """"likes'' using Utterances (<https://utteranc.es>), dynamic badges reflecting the status as the context evolved, live updates during the event with the announced awardees in a matter of a pull-request.

  
In this talk, I will go through the main features of the package and its application for the CovidR contest, highlighting the power of using open source tools for community and collaborative initiatives.",Community and Outreach,9B Community and Outreach 2
2021,FLAME: Interpretable Matching for Causal Inference,Regular talks > Track C,UseR! 2021,UseR! 2021,"Orlandi, Vittorio Dominic",The Lounge #talk_stats_data_analysis_2,"Matching methods are a class of techniques for estimating casual effects from observational data. Such methods match similar units together to emulate the randomization achieved by controlled experiments. Crucially, matching methods rely on a distance measure to determine similarity and thereby match units together. In this talk, we present an R package, FLAME, implementing the Fast, Large-scale Almost Matching Exactly (FLAME) and Dynamic Almost Matching Exactly (DAME) algorithms for performing matching on categorical datasets. These algorithms learn a weighted Hamming distance metric via machine learning on a held out dataset and match units directly on covariate values, prioritizing matches on more important covariates. The R package features an efficient bit-vectors implementation, allowing it to scale to datasets with hundreds of thousands of units and dozens of covariates, with a database implementation under development that allows it to operate on datasets too large to fit in memory. FLAME provides easy summarization, analysis, and visualization of treatment effect estimates, and features a wide variety of options for how matching is to be performed, allowing for users to make analysis-specific decisions throughout the matching procedure. We present an overview of the main functionality of the package and then illustrate an application to the 2010 US NCHS Natality Dataset, in which we study the effect of smoking during pregnancy on NICU admissions.","Bayesian models, Social sciences, Statistical models, Time series",9C - Statistical modeling and data analysis 2
2021,A little bit about RStudio,Regular talks > Track A,UseR! 2021,UseR! 2021,"Rickert, Joseph",The Lounge #talk_r_production_3,TBA,"R in production, R in the wild (unusual applications of R)",9A - R in production 3
2021,How open-source and R enable state-of-the-art media production,Regular talks > Track B,UseR! 2021,UseR! 2021,"Nantz, Eric",The Lounge #talk_community_outreach_2,"The global pandemic has impacted numerous aspects of lives, with many of us learning new techniques for harnessing digital technology for our daily work and communicating with the rest of the world. The landscape of software for producing content in both audio and video has seen tremendous growth in capabilities, creating new opportunities to share knowledge. In what seems like a lifetime ago, I created the R-Podcast in 2011 as a unique way to share my journey in learning more about the R language, providing resources to new and experienced users alike, and to showcase the awesome work of the R community. Even in that early stage, I was able to harness open-source software to produce and distribute the episodes, even with R playing a prominent role in the web presence. More recently, I have launched a new venture called the Shiny Developer Series that leverages the latest media production open-source software such as Open Broadcaster Software (OBS) and OBS Ninja to give me full control to produce my content without any compromises. In this talk, I will share my key learnings from producing media content about R and how myself and the growing community of R media content producers are using open-source to spread their knowledge of R to the entire world.",Community and Outreach,9B Community and Outreach 2
2021,Questions,Regular talks > Track C,UseR! 2021,UseR! 2021,All presenters in the session,The Lounge #talk_stats_data_analysis_2,Last questions for all the speakers,Community and Outreach,1A - Community and Outreach 1
